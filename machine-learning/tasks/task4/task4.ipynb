{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score, normalized_mutual_info_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score, v_measure_score, silhouette_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from numpy.linalg import inv, pinv\n",
    "from kneed import KneeLocator\n",
    "import warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 Data Preprocessing and Exploratory Data Analysis\n",
    "\n",
    "We perform the following steps:\n",
    "1. Load the dataset (\"Dataset.csv\") and verify its integrity.\n",
    "2. Confirm that there are no missing values.\n",
    "3. Identify and analyze outliers using visualizations such as boxplots.\n",
    "4. Visualize feature distributions with histograms and KDE plots to understand the\n",
    "overall distribution of each feature.\n",
    "5. Review feature statistics (e.g., mean, standard deviation) to get insights into the\n",
    "data.\n",
    "6. Normalize or standardize the dataset so that all features contribute equally in\n",
    "distance calculations, which is crucial for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 1: Load the dataset (\"Dataset.csv\") and verify its integrity.\n",
    "\n",
    "Manual inspection of the dataset determined that there are 900 rows (excluding the header row) and 8 columns. There to satisfy the integrity requirement we take that to mean the row and column counts are equal after the dataframe is loaded.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset.csv\") # load the dataset\n",
    "rows, cols = df.shape # get the row and column counts\n",
    "print(f\"Dataset shape: {rows} rows, {cols} columns\") \n",
    "\n",
    "# programmatic verification of the integrity of the dataset, throw an error if the row or column counts are not equal to 900 and 8 respectively\n",
    "if rows != 900:\n",
    "    assert False, \"The number of rows in the dataset is not equal to 900\"\n",
    "if cols != 8:\n",
    "    assert False, \"The number of columns in the dataset is not equal to 8\"\n",
    "\n",
    "print(\"Dataset integrity verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 2: Confirm that there are no missing values.\n",
    "Count the number of missing values in each column and throw an error if any are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = df.isnull().sum()\n",
    "if missing_values_count.sum() > 0:\n",
    "    assert False, \"The dataset contains missing values!!!! FIX\"\n",
    "print(\"Good, No missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 3: Identify and analyze outliers using visualizations such as boxplots.\n",
    "Boxplots for each numerical feature to identify and analyze outliers. Calculate and display statistics about potential outliers. This can be done by calculating the IQR and then using that to identify the lower and upper bounds of the outliers.\n",
    "The label is categorical so not included in outlier detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_palette('viridis') # set colour scheme\n",
    "\n",
    "# Get numerical features from the dataset\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Create boxplots for each numerical feature\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(3, 3, i+1)  # Adjust grid based on number of features\n",
    "    sns.boxplot(y=df[feature])\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Boxplots for Numerical Features to Identify Outliers', fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display statistics about outliers\n",
    "print(\"Potential outliers analysis:\")\n",
    "for feature in numerical_features:\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "\n",
    "    # A standard way to detect outliers is to use the IQR (Interquartile Range) then outliers are any values that fall outside of 1.5 times the IQR below Q1 or above Q3\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)][feature]\n",
    "    print(f\"{feature}: {len(outliers)} outliers detected\")\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"  - Min Boundary: {outliers.min():.2f}, Max Boundary {outliers.max():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 4: Visualise feature distributions with histograms and KDE plots to understand the overall distribution of each feature.\n",
    "\n",
    "Seaborn has differing functions for histograms and KDE plots. Use these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns # list of numerical features\n",
    "\n",
    "# Display histograms for each numerical feature\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(3, 3, i+1)  # Adjust grid based on number of features\n",
    "    sns.histplot(df[feature])\n",
    "    plt.title(f'Histogram of {feature}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Feature Distributions with Histograms', fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "# Create KDE plots for each numerical feature\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Get numerical features from the dataset\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Create KDE plots for each numerical feature\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(3, 3, i+1)  # Adjust grid based on number of features\n",
    "    sns.kdeplot(df[feature], fill=True)\n",
    "    plt.title(f'KDE Plot of {feature}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Feature Distributions with KDE Plots', fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features are skewed to either the left or right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 5 - Review feature statistics (e.g., mean, standard deviation) to get insights into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Basic Statistics for Numerical Features via Pandas Dataframe describe:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Calculate additional statistics that aren't in describe()\n",
    "print(\"\\nAdditional Statistics:\")\n",
    "numerical_stats = pd.DataFrame({\n",
    "    'Median': df.select_dtypes(include=[np.number]).median(),\n",
    "    'Skewness': df.select_dtypes(include=[np.number]).skew(),\n",
    "    'Kurtosis': df.select_dtypes(include=[np.number]).kurt(),\n",
    "    'IQR': df.select_dtypes(include=[np.number]).quantile(0.75) - df.select_dtypes(include=[np.number]).quantile(0.25),\n",
    "    'Range': df.select_dtypes(include=[np.number]).max() - df.select_dtypes(include=[np.number]).min()\n",
    "})\n",
    "display(numerical_stats)\n",
    "\n",
    "# Generate a correlation matrix\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "display(correlation_matrix)\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that lengths and areas are highly correlated, which is expected as area is a function of length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 6 - Normalize or standardize the dataset so that all features contribute equally in distance calculations, which is crucial for clustering.\n",
    "\n",
    "For every numeric feature, we will normalize it to a range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns.tolist() # create list of numerical columns\n",
    "scaler = MinMaxScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns]) # fit then transform the numerical columns\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Impact of the Number of Clusters on KMeans Clustering with Euclidean Distance\n",
    "\n",
    "The subtask for this are:\n",
    "1. Apply KMeans clustering (using Euclidean distance) on the standardized dataset.\n",
    "2. For a range of cluster numbers (e.g., from 1 to 10), compute the inertia (SSE) and plot\n",
    "these values to identify the “elbow” point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(\n",
    "    model, k=(1,11), metric='distortion', timings=False\n",
    ") #distortion same as Euclidean distance\n",
    "\n",
    "visualizer.fit(df[numerical_columns])        # Fit the data to the visualizer\n",
    "visualizer.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, the elbow appears to be when the cluster number is 5 as after that point the inertia decreases at a slower rate than for lower cluster numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Evaluating the Stability of KMeans and KMeans++ Initialization\n",
    "\n",
    "Subtasks are:\n",
    "1. Run KMeans clustering 50 times using two initialization methods:\n",
    "    - Standard random initialization.\n",
    "    - KMeans++ initialization.\n",
    "2. Compute and compare the average inertia (SSE) and the Silhouette Score for each\n",
    "method over these iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_iterations = 50\n",
    "n_clusters = 3  # Using 5 clusters based on the elbow method from previous task\n",
    "random_inertias = []\n",
    "random_silhouette_scores = []\n",
    "kmeans_plus_inertias = []\n",
    "kmeans_plus_silhouette_scores = []\n",
    "\n",
    "for i in range(n_iterations): # iterate 50 times\n",
    "\n",
    "    # random init kmeans\n",
    "    kmeans_random = KMeans(n_clusters=n_clusters, init='random', random_state=i)\n",
    "    kmeans_random.fit(df[numerical_columns])\n",
    "    random_inertias.append(kmeans_random.inertia_)\n",
    "    \n",
    "    # random init silhouette score\n",
    "    labels_random = kmeans_random.labels_\n",
    "    random_silhouette_scores.append(silhouette_score(df[numerical_columns], labels_random))\n",
    "    \n",
    "    # KMeans++ initialisation\n",
    "    kmeans_plus = KMeans(n_clusters=n_clusters, init='k-means++', random_state=i)\n",
    "    kmeans_plus.fit(df[numerical_columns])\n",
    "    kmeans_plus_inertias.append(kmeans_plus.inertia_)\n",
    "    \n",
    "    # calc silhouette score for kmeans++\n",
    "    labels_plus = kmeans_plus.labels_\n",
    "    kmeans_plus_silhouette_scores.append(silhouette_score(df[numerical_columns], labels_plus))\n",
    "\n",
    "# calc average metrics\n",
    "avg_random_inertia = np.mean(random_inertias)\n",
    "avg_random_silhouette = np.mean(random_silhouette_scores)\n",
    "avg_kmeans_plus_inertia = np.mean(kmeans_plus_inertias)\n",
    "avg_kmeans_plus_silhouette = np.mean(kmeans_plus_silhouette_scores)\n",
    "\n",
    "# calc standard deviations \n",
    "std_random_inertia = np.std(random_inertias)\n",
    "std_random_silhouette = np.std(random_silhouette_scores)\n",
    "std_kmeans_plus_inertia = np.std(kmeans_plus_inertias)\n",
    "std_kmeans_plus_silhouette = np.std(kmeans_plus_silhouette_scores)\n",
    "\n",
    "# Display results\n",
    "print(\"Standard Random Initialisation:\")\n",
    "print(f\"Average Inertia: {avg_random_inertia:.2f} (±{std_random_inertia:.2f})\")\n",
    "print(f\"Average Silhouette Score: {avg_random_silhouette:.4f} (±{std_random_silhouette:.4f})\")\n",
    "print(\"\\nKMeans++ Initialisation:\")\n",
    "print(f\"Average Inertia: {avg_kmeans_plus_inertia:.2f} (±{std_kmeans_plus_inertia:.2f})\")\n",
    "print(f\"Average Silhouette Score: {avg_kmeans_plus_silhouette:.4f} (±{std_kmeans_plus_silhouette:.4f})\")\n",
    "\n",
    "# Plot the distribution of inertias for both methods\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(random_inertias, alpha=0.7, label='Random Init')\n",
    "plt.hist(kmeans_plus_inertias, alpha=0.7, label='KMeans++ Init')\n",
    "plt.xlabel('Inertia')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Inertia Values')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(random_silhouette_scores, alpha=0.7, label='Random Init')\n",
    "plt.hist(kmeans_plus_silhouette_scores, alpha=0.7, label='KMeans++ Init')\n",
    "plt.xlabel('Silhouette Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Silhouette Scores')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above, kmeans++ is slightly more susceptible to differences in initialisation values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 4 - Clustering Evaluation Using Purity and Mutual Information\n",
    "\n",
    "Subtasks are:\n",
    "\n",
    "1. Use KMeans (with the optimal k from Question 2) to cluster the data. Assume the\n",
    "dataset contains a ground-truth label column (e.g., \"label\"). For each cluster, assign a\n",
    "label based on the majority class.\n",
    "2. Evaluation Metrics: Compute and report the following:\n",
    "    1. Purity Score: Measures how homogeneous each cluster is relative to the true\n",
    "labels.\n",
    "    2. Mutual Information Score: Quantifies the mutual dependence between the\n",
    "clustering results and the true labels.\n",
    "    3. Silhouette Score: Evaluates the clustering quality without reference to the\n",
    "ground truth by comparing intra-cluster cohesion versus inter-cluster\n",
    "separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 was the optimal k from task 2\n",
    "k = 3\n",
    "\n",
    "# get labels and features\n",
    "X = df.drop('label', axis=1)\n",
    "true_labels = df['label']\n",
    "\n",
    "# KMeans with the optimal k\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, init='k-means++')\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# based on majority class assign a label to each cluster\n",
    "cluster_to_label = {}\n",
    "for cluster_id in range(k):\n",
    "    cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "    cluster_true_labels = true_labels.iloc[cluster_indices]\n",
    "    # get the most common label\n",
    "    most_common_label = Counter(cluster_true_labels).most_common(1)[0][0]\n",
    "    cluster_to_label[cluster_id] = most_common_label\n",
    "\n",
    "# print mappings\n",
    "print(\"Cluster to Label Mapping:\")\n",
    "for cluster_id, label in cluster_to_label.items():\n",
    "    print(f\"Cluster {cluster_id} has label: {label}\")\n",
    "\n",
    "# calc purity score\n",
    "def purity_score(y_true, y_pred):\n",
    "    contingency_matrix = np.zeros((k, len(np.unique(y_true))))\n",
    "    \n",
    "    for i in range(len(y_true)):\n",
    "        true_label_idx = np.where(np.unique(y_true) == y_true.iloc[i])[0][0]\n",
    "        contingency_matrix[y_pred[i], true_label_idx] += 1\n",
    "    \n",
    "    return np.sum(np.max(contingency_matrix, axis=1)) / len(y_true)\n",
    "\n",
    "# calc evaluation metrics\n",
    "purity = purity_score(true_labels, cluster_labels)\n",
    "mutual_info = normalized_mutual_info_score(true_labels, cluster_labels)\n",
    "silhouette = silhouette_score(X, cluster_labels)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Purity Score: {purity:.4f}\")\n",
    "print(f\"Normalized Mutual Information Score: {mutual_info:.4f}\")\n",
    "print(f\"Silhouette Score: {silhouette:.4f}\")\n",
    "\n",
    "# Graph clusters\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for cluster_id in range(k):\n",
    "    cluster_points = X[cluster_labels == cluster_id]\n",
    "    plt.scatter(cluster_points.iloc[:, 0], cluster_points.iloc[:, 1], \n",
    "                label=f\"Cluster {cluster_id}: {cluster_to_label[cluster_id]}\")\n",
    "\n",
    "plt.title('Clusters by KMeans')\n",
    "plt.xlabel(X.columns[0])\n",
    "plt.ylabel(X.columns[1])\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for label in np.unique(true_labels):\n",
    "    label_points = X[true_labels == label]\n",
    "    plt.scatter(label_points.iloc[:, 0], label_points.iloc[:, 1], label=label)\n",
    "\n",
    "plt.title('True Labels')\n",
    "plt.xlabel(X.columns[0])\n",
    "plt.ylabel(X.columns[1])\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 Principal Component Analysis (PCA) for Dimensionality Reduction\n",
    "\n",
    "Subtasks are:\n",
    "1. Apply PCA to reduce the dataset to 4 principal components.\n",
    "2. Plot the cumulative variance explained by the principal components and determine\n",
    "how many components are needed to retain 90% of the total variance.\n",
    "3. Create a 3D scatter plot of the first three principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce the dataset to 4 principal components\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Subtask 1\n",
    "\n",
    "# Apply PCA with 4 components\n",
    "pca = PCA(n_components=4)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "\n",
    "pca_df = pd.DataFrame(\n",
    "    data=X_pca,\n",
    "    columns=['PC1', 'PC2', 'PC3', 'PC4']\n",
    ")\n",
    "\n",
    "pca_df['label'] = true_labels\n",
    "\n",
    "\n",
    "\n",
    "# Subtask 2 - Graph the explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, label='Individual explained variance')\n",
    "plt.step(range(1, len(cumulative_variance) + 1), cumulative_variance, where='mid', label='Cumulative explained variance')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='90% threshold')\n",
    "\n",
    "# Calc how many components needed for 90% variance\n",
    "components_needed = np.argmax(cumulative_variance >= 0.9) + 1\n",
    "plt.axvline(x=components_needed, color='g', linestyle='--', \n",
    "            label=f'{components_needed} components needed for 90% variance')\n",
    "\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nExplained variance ratio by component:\")\n",
    "for i, var in enumerate(explained_variance):\n",
    "    print(f\"PC{i+1}: {var:.4f} ({cumulative_variance[i]:.4f} cumulative)\")\n",
    "\n",
    "print(f\"\\nNumber of components needed to retain 90% variance: {components_needed}\")\n",
    "\n",
    "# Make a 3D scatter plot of the first three principal components\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "unique_labels = np.unique(true_labels)\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    mask = pca_df['label'] == label\n",
    "    ax.scatter(\n",
    "        pca_df.loc[mask, 'PC1'],\n",
    "        pca_df.loc[mask, 'PC2'],\n",
    "        pca_df.loc[mask, 'PC3'],\n",
    "        label=label,\n",
    "        color=color,\n",
    "        alpha=0.7,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "ax.set_title('3D Projection of First Three Principal Components')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6 - Density Based Clustering Using DBSCAN with Different Distance Metrics\n",
    "\n",
    "Subtasks are:\n",
    "\n",
    "1. Apply DBSCAN to the dataset twice:\n",
    "    1. Once using Euclidean distance.\n",
    "    2. Once using Mahalanobis distance.\n",
    "2. Determine the optimal values for eps (ε) and min_samples for each distance metric.\n",
    "3. Compare the clustering results from both distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_np = X_std # make a copy\n",
    "n_features = X_np.shape[1]\n",
    "\n",
    "# common starting heuristics\n",
    "min_samples_start = max(2, 2 * n_features) \n",
    "min_samples_end = min(30, int(X_np.shape[0] * 0.1)) \n",
    "min_samples_range = range(min_samples_start, min_samples_end + 1) \n",
    "\n",
    "print(f\"Will test min_samples in range: {list(min_samples_range)}\")\n",
    "print(\"---------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# function to find best parameters\n",
    "def find_best_dbscan_params(X, metric='euclidean', min_samples_range=range(5, 21),\n",
    "                            true_labels=None, inv_cov=None, kneed_S=1.0):\n",
    "\n",
    "    results = []\n",
    "    best_score = -1  \n",
    "    best_min_samples = None\n",
    "    best_eps = None\n",
    "\n",
    "    # Compute Mahalanobis distance matrix if needed\n",
    "    dist_matrix_mah = None\n",
    "    if metric == 'mahalanobis':\n",
    "        if inv_cov is None:\n",
    "            raise ValueError(\"Inverse covariance matrix 'inv_cov' must be provided for Mahalanobis metric.\")\n",
    "        try:\n",
    "            dist_condensed_mah = pdist(X, metric='mahalanobis', VI=inv_cov)\n",
    "            dist_matrix_mah = squareform(dist_condensed_mah)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating Mahalanobis distance matrix: {e}\")\n",
    "            return None, None, -1, []\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Param search for {metric.capitalize()} Distance ---\")\n",
    "    for min_samples in min_samples_range:\n",
    "        print(f\"Testing min_samples = {min_samples}...\", end=\"\")\n",
    "        k = min_samples - 1 # index for k-distance plot\n",
    "\n",
    "        try:\n",
    "            # 1. calc k-distances\n",
    "            if metric == 'euclidean':\n",
    "                nn = NearestNeighbors(n_neighbors=min_samples, metric=metric)\n",
    "                nn.fit(X)\n",
    "                distances, _ = nn.kneighbors(X)\n",
    "                k_distances = np.sort(distances[:, k], axis=0)\n",
    "            elif metric == 'mahalanobis':\n",
    "                sorted_mah_distances = np.sort(dist_matrix_mah, axis=1)\n",
    "                if sorted_mah_distances.shape[1] > k+1:\n",
    "                     k_distances = np.sort(sorted_mah_distances[:, k + 1])\n",
    "                else:\n",
    "                     print(f\" Skipping min_samples={min_samples}, not enough neighbors in precomputed matrix.\")\n",
    "                     continue \n",
    "\n",
    "            # find elbow\n",
    "            if len(k_distances) == 0:\n",
    "                print(\" No k-distances calculated.\")\n",
    "                continue\n",
    "\n",
    "            kn = KneeLocator(\n",
    "                range(len(k_distances)),\n",
    "                k_distances,\n",
    "                curve='concave',\n",
    "                direction='increasing',\n",
    "                S=kneed_S \n",
    "            )\n",
    "\n",
    "            current_eps = kn.elbow_y \n",
    "\n",
    "            if current_eps is None:\n",
    "                print(f\" Couldnt find elbow for min_samples={min_samples}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\" Auto eps = {current_eps:.4f}.\", end=\"\")\n",
    "\n",
    "            #  Run DBSCAN\n",
    "            if metric == 'euclidean':\n",
    "                db = DBSCAN(eps=current_eps, min_samples=min_samples, metric=metric)\n",
    "                labels = db.fit_predict(X)\n",
    "            elif metric == 'mahalanobis':\n",
    "                db = DBSCAN(eps=current_eps, min_samples=min_samples, metric='precomputed')\n",
    "                labels = db.fit_predict(dist_matrix_mah)\n",
    "\n",
    "            # eval results\n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            n_noise = list(labels).count(-1)\n",
    "\n",
    "            # skip eval if only noise or one cluster found\n",
    "            if n_clusters < 1 : \n",
    "                 print(f\" Found < 1 cluster ({n_clusters}). Score = -1.\")\n",
    "                 score = -1.0 \n",
    "            elif n_clusters == 1 and n_noise == 0 and true_labels is None:\n",
    "                 print(f\" Found 1 cluster, no noise. Score = 0 (Silhouette undefined).\")\n",
    "                 score = 0.0\n",
    "            elif n_clusters < 2 and true_labels is None:\n",
    "                 print(f\" Found < 2 clusters ({n_clusters}). Silhouette Score invalid. Score = -1.\")\n",
    "                 score = -1.0\n",
    "            else:\n",
    "                if true_labels is not None:\n",
    "                    # Use ARI \n",
    "                    score = adjusted_rand_score(true_labels, labels)\n",
    "                    print(f\" Clusters={n_clusters}, Noise={n_noise}. ARI = {score:.3f}\")\n",
    "                else:\n",
    "                    # use Silhouette Score \n",
    "                    try:\n",
    "                        # filter out noise points\n",
    "                        if n_clusters >= 2:\n",
    "                            mask = labels != -1\n",
    "                            if np.sum(mask) > 1: \n",
    "                                score = silhouette_score(X[mask], labels[mask])\n",
    "                                print(f\" Clusters={n_clusters}, Noise={n_noise}. Silhouette = {score:.3f}\")\n",
    "                            else:\n",
    "                                score=-1.0 \n",
    "                                print(f\" Clusters={n_clusters}, Noise={n_noise}. Silhouette=N/A (too few points). Score = -1.\")\n",
    "                        else: \n",
    "                            score=-1.0\n",
    "                            print(f\" Clusters={n_clusters}, Noise={n_noise}. Silhouette=N/A (<2 clusters). Score = -1.\")\n",
    "\n",
    "                    except Exception as e_score:\n",
    "                        print(f\" Error calculating Silhouette score: {e_score}. Score = -1.\")\n",
    "                        score = -1.0\n",
    "\n",
    "            results.append({'min_samples': min_samples, 'eps': current_eps, 'score': score,\n",
    "                            'n_clusters': n_clusters, 'n_noise': n_noise})\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_min_samples = min_samples\n",
    "                best_eps = current_eps\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing min_samples={min_samples}: {e}\")\n",
    "\n",
    "    print(f\"--- search done  for {metric.capitalize()} ---\")\n",
    "    if best_min_samples is not None:\n",
    "        print(f\"best score ({'ARI' if true_labels is not None else 'Silhouette'}): {best_score:.4f}\")\n",
    "        print(f\"best parameters: min_samples = {best_min_samples}, eps = {best_eps:.4f}\")\n",
    "    else:\n",
    "        print(\"couldnt find any valid clustering parameters.\")\n",
    "\n",
    "    return best_min_samples, best_eps, best_score, results\n",
    "\n",
    "\n",
    "\n",
    "# exec param search\n",
    "# euclidean\n",
    "best_min_samples_euc, best_eps_euc, best_score_euc, results_euc = find_best_dbscan_params(\n",
    "    X_np,\n",
    "    metric='euclidean',\n",
    "    min_samples_range=min_samples_range,\n",
    "    true_labels=true_labels\n",
    "    # kneed_S = 1.0 # Adjust sensitivity if needed\n",
    ")\n",
    "\n",
    "# mahalanobis\n",
    "inv_cov = None\n",
    "try:\n",
    "    cov = np.cov(X_np, rowvar=False)\n",
    "    inv_cov = inv(cov)\n",
    "    print(\"\\nUsing inverse of covariance matrix for Mahalanobis.\")\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"\\nCovariance matrix is singular. Using pseudo-inverse for Mahalanobis.\")\n",
    "    cov = np.cov(X_np, rowvar=False)\n",
    "    inv_cov = pinv(cov)\n",
    "\n",
    "best_min_samples_mah, best_eps_mah, best_score_mah, results_mah = None, None, -1, []\n",
    "if inv_cov is not None:\n",
    "     best_min_samples_mah, best_eps_mah, best_score_mah, results_mah = find_best_dbscan_params(\n",
    "         X_np,\n",
    "         metric='mahalanobis',\n",
    "         min_samples_range=min_samples_range,\n",
    "         true_labels=true_labels,\n",
    "         inv_cov=inv_cov\n",
    "     )\n",
    "else:\n",
    "    print(\"\\nskipping mahalanobis search due to error calculating inverse covariance.\")\n",
    "\n",
    "\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "\n",
    "print(\"Applying DBSCAN with best found parameters...\")\n",
    "labels_euc, labels_mah = None, None \n",
    "\n",
    "# euclidean DBSCAN for best params\n",
    "if best_min_samples_euc is not None and best_eps_euc is not None:\n",
    "    print(f\"best and  final Euclidean DBSCAN with min_samples={best_min_samples_euc}, eps={best_eps_euc:.4f}\")\n",
    "    dbscan_euc = DBSCAN(eps=best_eps_euc, min_samples=best_min_samples_euc, metric='euclidean')\n",
    "    labels_euc = dbscan_euc.fit_predict(X_np)\n",
    "else:\n",
    "    print(\"Could not run final Euclidean DBSCAN: Best parameters not found.\")\n",
    "\n",
    "# mahalanobis DBSCAN for best params\n",
    "if best_min_samples_mah is not None and best_eps_mah is not None and inv_cov is not None:\n",
    "    print(f\"best and final Mahalanobis DBSCAN with min_samples={best_min_samples_mah}, eps={best_eps_mah:.4f}\")\n",
    "\n",
    "    try:\n",
    "        dist_condensed_mah = pdist(X_np, metric='mahalanobis', VI=inv_cov)\n",
    "        dist_matrix_mah = squareform(dist_condensed_mah)\n",
    "        dbscan_mah = DBSCAN(eps=best_eps_mah, min_samples=best_min_samples_mah, metric='precomputed')\n",
    "        labels_mah = dbscan_mah.fit_predict(dist_matrix_mah)\n",
    "    except Exception as e:\n",
    "         print(f\"Error running final Mahalanobis DBSCAN: {e}\")\n",
    "         labels_mah = None \n",
    "else:\n",
    "    print(\"couldnt run final Mahalanobis DBSCAN: Best parameters not found or inv_cov missing.\")\n",
    "\n",
    "print(\"final DBSCAN fitting complete.\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "\n",
    "\n",
    "# compare clusters\n",
    "\n",
    "print(\"comparing best found params.....\")\n",
    "\n",
    "# analyse labels\n",
    "def analyse_dbscan_labels(labels, method_name, X_data, true_labels_comp):\n",
    "    if labels is None:\n",
    "        print(f\"--- {method_name} ---\")\n",
    "        print(\"clustering could not be performed.\")\n",
    "        print(\"-\" * 15)\n",
    "        return\n",
    "\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    print(f\"--- {method_name} ---\")\n",
    "    print(f\"Estimated number of clusters: {n_clusters}\")\n",
    "    print(f\"Estimated number of noise points: {n_noise} ({n_noise / len(labels):.2%})\")\n",
    "\n",
    "    try:\n",
    "        if true_labels_comp is not None and len(labels) == len(true_labels_comp):\n",
    "             ari = adjusted_rand_score(true_labels_comp, labels)\n",
    "             v_measure = v_measure_score(true_labels_comp, labels)\n",
    "             print(f\"Adjusted Rand Index (ARI): {ari:.3f}\")\n",
    "             print(f\"V-Measure: {v_measure:.3f}\")\n",
    "        elif true_labels_comp is not None:\n",
    "             print(\"Skipping comparison metrics: true_labels length mismatch.\")\n",
    "\n",
    "        if n_clusters >= 2:\n",
    "             mask = labels != -1\n",
    "             if np.sum(mask) > 1:\n",
    "                 sil_score = silhouette_score(X_data[mask], labels[mask])\n",
    "                 print(f\"Silhouette Score (excluding noise): {sil_score:.3f}\")\n",
    "             else:\n",
    "                 print(\"Silhouette Score: Not enough non-noise points.\")\n",
    "        else:\n",
    "            print(\"Silhouette Score: Requires >= 2 clusters.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during label analysis: {e}\")\n",
    "    print(\"-\" * 15)\n",
    "\n",
    "# Analyse results\n",
    "analyse_dbscan_labels(labels_euc, \"Euclidean DBSCAN (Best Params)\", X_np, true_labels)\n",
    "analyse_dbscan_labels(labels_mah, \"Mahalanobis DBSCAN (Best Params)\", X_np, true_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7 - Clustering Performance on PCA-Reduced v Full Dataset\n",
    "\n",
    "1. Apply KMeans clustering to:\n",
    "    1. The original standardized dataset.\n",
    "    2. The PCA-transformed dataset (using the principal components from Question5).\n",
    "2.  Evaluate the clustering quality using the Silhouette Score.\n",
    "3. Compare whether the PCA-transformed dataset results in better-separated and more compact clusters relative to the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to run kmeans and eval silhouette and inertia score\n",
    "def apply_kmeans_and_evaluate(data, name, n_clusters_range=range(2, 11)):\n",
    "    results = []\n",
    "    \n",
    "    for k in n_clusters_range:\n",
    "        # Apply KMeans\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(data)\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        sil_score = silhouette_score(data, cluster_labels)\n",
    "        \n",
    "        # Calculate inertia (sum of squared distances to closest centroid)\n",
    "        inertia = kmeans.inertia_\n",
    "        \n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'silhouette': sil_score,\n",
    "            'inertia': inertia,\n",
    "            'labels': cluster_labels\n",
    "        })\n",
    "        \n",
    "        print(f\"{name} with {k} clusters - Silhouette Score: {sil_score:.3f}, Inertia: {inertia:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# original dataset\n",
    "original_results = apply_kmeans_and_evaluate(X_std, \"Original Dataset\")\n",
    "\n",
    "# pca dataset\n",
    "pca_results = apply_kmeans_and_evaluate(X_pca, \"PCA Dataset\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([r['k'] for r in original_results], [r['silhouette'] for r in original_results], 'o-', label='Original Dataset')\n",
    "plt.plot([r['k'] for r in pca_results], [r['silhouette'] for r in pca_results], 'o-', label='PCA Dataset')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "# find the best on silhouette score\n",
    "best_k_original = max(original_results, key=lambda x: x['silhouette'])\n",
    "best_k_pca = max(pca_results, key=lambda x: x['silhouette'])\n",
    "\n",
    "print(\"\\nBest results:\")\n",
    "print(f\"Original Dataset: k={best_k_original['k']}, Silhouette Score={best_k_original['silhouette']:.3f}\")\n",
    "print(f\"PCA Dataset: k={best_k_pca['k']}, Silhouette Score={best_k_pca['silhouette']:.3f}\")\n",
    "\n",
    "# determine which is better\n",
    "if best_k_pca['silhouette'] > best_k_original['silhouette']:\n",
    "    print(\"\\nPCA dataset is better.\")\n",
    "else:\n",
    "    print(\"\\nOriginal dataset is better.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset's best silhouette score is 0.441. The PCA dataset's best silhoutte score is 0.442. Therefore the PCA dataset results in a better separated and more compact clusters, however the difference between the results is very minor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8 - Clustering Using t-SNE\n",
    "\n",
    "1. Apply t-SNE (using the exact method) to reduce the dataset to 4 components.\n",
    "2. Create a 3D scatter plot of the first three t-SNE components.\n",
    "3. Apply KMeans clustering on the t‐SNE–reduced data using an appropriate number of\n",
    "clusters (e.g., based on prior optimal k or an elbow method on the t‐SNE output).\n",
    "4. Evaluate the clustering performance on the t‐SNE–reduced data using metrics such\n",
    "as the Silhouette Score and compare these results to clustering on the original and\n",
    "PCA–transformed dataset.\n",
    "5. Discuss whether the clusters formed on the t-SNE–reduced data are more distinct\n",
    "and how well they correspond to the known data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute t-SNE \n",
    "tsne = TSNE(n_components=4, method='exact', random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_std)\n",
    "\n",
    "# 3D scatter plot of the first three components\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "if 'label' in df.columns:\n",
    "    labels = df['label'].unique()\n",
    "    for i, label in enumerate(labels):\n",
    "        indices = df['label'] == label\n",
    "        ax.scatter(X_tsne[indices, 0], X_tsne[indices, 1], X_tsne[indices, 2], \n",
    "                   label=label, alpha=0.7)\n",
    "else:\n",
    "    ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2], alpha=0.7)\n",
    "\n",
    "ax.set_title('3D t-SNE Visualisation')\n",
    "ax.set_xlabel('t-SNE Component 1')\n",
    "ax.set_ylabel('t-SNE Component 2')\n",
    "ax.set_zlabel('t-SNE Component 3')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Apply KMeans clustering on the t-SNE-reduced data\n",
    "# First, determine the optimal number of clusters using the elbow method\n",
    "distortions = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_tsne)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "    \n",
    "    labels = kmeans.labels_\n",
    "    silhouette_avg = silhouette_score(X_tsne, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "\n",
    "# calc silhouette scores\n",
    "tsne_results = []\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_tsne)\n",
    "    silhouette_avg = silhouette_score(X_tsne, labels)\n",
    "    tsne_results.append({'k': k, 'silhouette': silhouette_avg})\n",
    "\n",
    "best_k_tsne = max(tsne_results, key=lambda x: x['silhouette'])\n",
    "\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Original Dataset: k={best_k_original['k']}, Silhouette Score={best_k_original['silhouette']:.3f}\")\n",
    "print(f\"PCA Dataset: k={best_k_pca['k']}, Silhouette Score={best_k_pca['silhouette']:.3f}\")\n",
    "print(f\"t-SNE Dataset: k={best_k_tsne['k']}, Silhouette Score={best_k_tsne['silhouette']:.3f}\")\n",
    "\n",
    "# find the best\n",
    "best_method = max([\n",
    "    ('Original', best_k_original['silhouette']),\n",
    "    ('PCA', best_k_pca['silhouette']),\n",
    "    ('t-SNE', best_k_tsne['silhouette'])\n",
    "], key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nThe {best_method[0]} dataset provides better clustering quality with a silhouette score of {best_method[1]:.3f}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-SNE dataset has the lowest silhouette score, therefore it has the least well defined cluster. t-SNE (t-Distributed Stochastic Neighbour Embedding). Even though t-SNE is a dimensionality reduction technique which theoretically could improve cluster cohesion, the original dataset has only 8 features and therefore may not possess enough dimensions to benefit from this technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
