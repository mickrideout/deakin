{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 Data Preprocessing and Exploratory Data Analysis\n",
    "\n",
    "We perform the following steps:\n",
    "1. Load the dataset (\"Dataset.csv\") and verify its integrity.\n",
    "2. Confirm that there are no missing values.\n",
    "3. Identify and analyze outliers using visualizations such as boxplots.\n",
    "4. Visualize feature distributions with histograms and KDE plots to understand the\n",
    "overall distribution of each feature.\n",
    "5. Review feature statistics (e.g., mean, standard deviation) to get insights into the\n",
    "data.\n",
    "6. Normalize or standardize the dataset so that all features contribute equally in\n",
    "distance calculations, which is crucial for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 1: Load the dataset (\"Dataset.csv\") and verify its integrity.\n",
    "\n",
    "Manual inspection of the dataset determined that there are 900 rows (excluding the header row) and 8 columns. There to satisfy the integrity requirement we take that to mean the row and column counts are equal after the dataframe is loaded.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset.csv\") # load the dataset\n",
    "rows, cols = df.shape # get the row and column counts\n",
    "print(f\"Dataset shape: {rows} rows, {cols} columns\") \n",
    "\n",
    "# programmatic verification of the integrity of the dataset, throw an error if the row or column counts are not equal to 900 and 8 respectively\n",
    "if rows != 900:\n",
    "    assert False, \"The number of rows in the dataset is not equal to 900\"\n",
    "if cols != 8:\n",
    "    assert False, \"The number of columns in the dataset is not equal to 8\"\n",
    "\n",
    "print(\"Dataset integrity verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 2: Confirm that there are no missing values.\n",
    "Count the number of missing values in each column and throw an error if any are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = df.isnull().sum()\n",
    "if missing_values_count.sum() > 0:\n",
    "    assert False, \"The dataset contains missing values!!!! FIX\"\n",
    "print(\"Good, No missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 3: Identify and analyze outliers using visualizations such as boxplots.\n",
    "Boxplots for each numerical feature to identify and analyze outliers. Calculate and display statistics about potential outliers. This can be done by calculating the IQR and then using that to identify the lower and upper bounds of the outliers.\n",
    "The label is categorical so not included in outlier detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_palette('viridis') # set colour scheme\n",
    "\n",
    "# Get numerical features from the dataset\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Create boxplots for each numerical feature\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(3, 3, i+1)  # Adjust grid based on number of features\n",
    "    sns.boxplot(y=df[feature])\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Boxplots for Numerical Features to Identify Outliers', fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display statistics about outliers\n",
    "print(\"Potential outliers analysis:\")\n",
    "for feature in numerical_features:\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "\n",
    "    # A standard way to detect outliers is to use the IQR (Interquartile Range) then outliers are any values that fall outside of 1.5 times the IQR below Q1 or above Q3\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)][feature]\n",
    "    print(f\"{feature}: {len(outliers)} outliers detected\")\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"  - Min Boundary: {outliers.min():.2f}, Max Boundary {outliers.max():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 4: Visualise feature distributions with histograms and KDE plots to understand the overall distribution of each feature.\n",
    "\n",
    "Seaborn has differing functions for histograms and KDE plots. Use these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns # list of numerical features\n",
    "\n",
    "# Display histograms for each numerical feature\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(3, 3, i+1)  # Adjust grid based on number of features\n",
    "    sns.histplot(df[feature])\n",
    "    plt.title(f'Histogram of {feature}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Feature Distributions with Histograms', fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "# Create KDE plots for each numerical feature\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Get numerical features from the dataset\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Create KDE plots for each numerical feature\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(3, 3, i+1)  # Adjust grid based on number of features\n",
    "    sns.kdeplot(df[feature], fill=True)\n",
    "    plt.title(f'KDE Plot of {feature}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Feature Distributions with KDE Plots', fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features are skewed to either the left or right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 5 - Review feature statistics (e.g., mean, standard deviation) to get insights into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Basic Statistics for Numerical Features via Pandas Dataframe describe:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Calculate additional statistics that aren't in describe()\n",
    "print(\"\\nAdditional Statistics:\")\n",
    "numerical_stats = pd.DataFrame({\n",
    "    'Median': df.select_dtypes(include=[np.number]).median(),\n",
    "    'Skewness': df.select_dtypes(include=[np.number]).skew(),\n",
    "    'Kurtosis': df.select_dtypes(include=[np.number]).kurt(),\n",
    "    'IQR': df.select_dtypes(include=[np.number]).quantile(0.75) - df.select_dtypes(include=[np.number]).quantile(0.25),\n",
    "    'Range': df.select_dtypes(include=[np.number]).max() - df.select_dtypes(include=[np.number]).min()\n",
    "})\n",
    "display(numerical_stats)\n",
    "\n",
    "# Generate a correlation matrix\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "display(correlation_matrix)\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that lengths and areas are highly correlated, which is expected as area is a function of length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 6 - Normalize or standardize the dataset so that all features contribute equally in distance calculations, which is crucial for clustering.\n",
    "\n",
    "For every numeric feature, we will normalize it to a range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns.tolist() # create list of numerical columns\n",
    "scaler = MinMaxScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns]) # fit then transform the numerical columns\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Impact of the Number of Clusters on KMeans Clustering with Euclidean Distance\n",
    "\n",
    "The subtask for this are:\n",
    "1. Apply KMeans clustering (using Euclidean distance) on the standardized dataset.\n",
    "2. For a range of cluster numbers (e.g., from 1 to 10), compute the inertia (SSE) and plot\n",
    "these values to identify the “elbow” point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inertia_values = []\n",
    "cluster_range = range(1, 11) \n",
    "\n",
    "# Try kmeans on 1 to 10 clusters and store the inertia values for each\n",
    "for k in cluster_range:\n",
    "    kmeans_model = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans_model.fit(df[numerical_columns])\n",
    "    inertia_values.append(kmeans_model.inertia_)\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cluster_range, inertia_values, 'bo-')\n",
    "plt.xlabel('Cluster Count)\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Kmeans Inertia for different cluster counts')\n",
    "plt.grid(True)\n",
    "plt.xticks(cluster_range)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, the elbow appears to be when the cluster number is 5 as after that point the inertia decreases at a slower rate than for lower cluster numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Evaluating the Stability of KMeans and KMeans++ Initialization\n",
    "\n",
    "Subtasks are:\n",
    "1. Run KMeans clustering 50 times using two initialization methods:\n",
    "    - Standard random initialization.\n",
    "    - KMeans++ initialization.\n",
    "2. Compute and compare the average inertia (SSE) and the Silhouette Score for each\n",
    "method over these iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Set parameters\n",
    "n_iterations = 50\n",
    "n_clusters = 5  # Using 5 clusters based on the elbow method from previous task\n",
    "random_inertias = []\n",
    "random_silhouette_scores = []\n",
    "kmeans_plus_inertias = []\n",
    "kmeans_plus_silhouette_scores = []\n",
    "\n",
    "# Run standard random initialisation 50 times\n",
    "for i in range(n_iterations):\n",
    "    # Standard random initialisation\n",
    "    kmeans_random = KMeans(n_clusters=n_clusters, init='random', random_state=i)\n",
    "    kmeans_random.fit(df[numerical_columns])\n",
    "    random_inertias.append(kmeans_random.inertia_)\n",
    "    \n",
    "    # Calculate silhouette score for random init\n",
    "    labels_random = kmeans_random.labels_\n",
    "    random_silhouette_scores.append(silhouette_score(df[numerical_columns], labels_random))\n",
    "    \n",
    "    # KMeans++ initialisation\n",
    "    kmeans_plus = KMeans(n_clusters=n_clusters, init='k-means++', random_state=i)\n",
    "    kmeans_plus.fit(df[numerical_columns])\n",
    "    kmeans_plus_inertias.append(kmeans_plus.inertia_)\n",
    "    \n",
    "    # Calculate silhouette score for kmeans++\n",
    "    labels_plus = kmeans_plus.labels_\n",
    "    kmeans_plus_silhouette_scores.append(silhouette_score(df[numerical_columns], labels_plus))\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_random_inertia = np.mean(random_inertias)\n",
    "avg_random_silhouette = np.mean(random_silhouette_scores)\n",
    "avg_kmeans_plus_inertia = np.mean(kmeans_plus_inertias)\n",
    "avg_kmeans_plus_silhouette = np.mean(kmeans_plus_silhouette_scores)\n",
    "\n",
    "# Calculate standard deviations to assess stability\n",
    "std_random_inertia = np.std(random_inertias)\n",
    "std_random_silhouette = np.std(random_silhouette_scores)\n",
    "std_kmeans_plus_inertia = np.std(kmeans_plus_inertias)\n",
    "std_kmeans_plus_silhouette = np.std(kmeans_plus_silhouette_scores)\n",
    "\n",
    "# Display results\n",
    "print(\"Standard Random Initialisation:\")\n",
    "print(f\"Average Inertia: {avg_random_inertia:.2f} (±{std_random_inertia:.2f})\")\n",
    "print(f\"Average Silhouette Score: {avg_random_silhouette:.4f} (±{std_random_silhouette:.4f})\")\n",
    "print(\"\\nKMeans++ Initialisation:\")\n",
    "print(f\"Average Inertia: {avg_kmeans_plus_inertia:.2f} (±{std_kmeans_plus_inertia:.2f})\")\n",
    "print(f\"Average Silhouette Score: {avg_kmeans_plus_silhouette:.4f} (±{std_kmeans_plus_silhouette:.4f})\")\n",
    "\n",
    "# Plot the distribution of inertias for both methods\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(random_inertias, alpha=0.7, label='Random Init')\n",
    "plt.hist(kmeans_plus_inertias, alpha=0.7, label='KMeans++ Init')\n",
    "plt.xlabel('Inertia')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Inertia Values')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(random_silhouette_scores, alpha=0.7, label='Random Init')\n",
    "plt.hist(kmeans_plus_silhouette_scores, alpha=0.7, label='KMeans++ Init')\n",
    "plt.xlabel('Silhouette Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Silhouette Scores')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
