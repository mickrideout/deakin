{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score, normalized_mutual_info_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score, v_measure_score, silhouette_score, davies_bouldin_score, mutual_info_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import pdist, squareform, mahalanobis\n",
    "from numpy.linalg import inv, pinv\n",
    "from kneed import KneeLocator\n",
    "import warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 Data Preprocessing and Exploratory Data Analysis\n",
    "\n",
    "We perform the following steps:\n",
    "1. Load the dataset (\"Dataset.csv\") and verify its integrity.\n",
    "2. Confirm that there are no missing values.\n",
    "3. Identify and analyze outliers using visualizations such as boxplots.\n",
    "4. Visualize feature distributions with histograms and KDE plots to understand the\n",
    "overall distribution of each feature.\n",
    "5. Review feature statistics (e.g., mean, standard deviation) to get insights into the\n",
    "data.\n",
    "6. Normalize or standardize the dataset so that all features contribute equally in\n",
    "distance calculations, which is crucial for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 1: Load the dataset (\"Dataset.csv\") and verify its integrity.\n",
    "\n",
    "Manual inspection of the dataset determined that there are 900 rows (excluding the header row) and 8 columns. There to satisfy the integrity requirement we take that to mean the row and column counts are equal after the dataframe is loaded.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Subtask 1 - Load the dataset (\"Dataset.csv\") and verify its integrity.\n",
    "#\n",
    "# Purpose: To load the required data from Dataset.csv into a pandas dataframe called 'df' and then to verify its \n",
    "#           integrity. Integrity is understood to mean that the correct number of rows and columns have been \n",
    "#           loaded that are in agreement with the manual inspection of these counts. \n",
    "#           If the row or column count is not in agreement with the manual count then the assertion will fail.\n",
    "# Takeaway: The datasets integrity has been validation otherwise execution will be stopped.\n",
    "########\n",
    "df = pd.read_csv(\"Dataset.csv\") # load the dataset\n",
    "rows, cols = df.shape # get the row and column counts\n",
    "print(f\"Dataset shape: {rows} rows, {cols} columns\") \n",
    "\n",
    "# programmatic verification of the integrity of the dataset, throw an error if the row or column counts are not equal to 900 and 8 respectively\n",
    "if rows != 900:\n",
    "    assert False, \"The number of rows in the dataset is not equal to 900\"\n",
    "if cols != 8:\n",
    "    assert False, \"The number of columns in the dataset is not equal to 8\"\n",
    "\n",
    "print(\"Dataset integrity verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 2: Confirm that there are no missing values.\n",
    "Count the number of missing values in each column and throw an error if any are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Subtask 2 - Confirm that there are no missing values.\n",
    "# \n",
    "# Purpose: To ensure that there are no missing values anywhere in the dataset. \n",
    "#          If there are missing values then the assertion will fail.\n",
    "# Takeaway: The dataset has no missing values otherwise execution will be stopped.\n",
    "########\n",
    "missing_values_count = df.isnull().sum()\n",
    "if missing_values_count.sum() > 0:\n",
    "    assert False, \"The dataset contains missing values!!!! FIX\"\n",
    "print(\"Good, No missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 3: Identify and analyze outliers using visualizations such as boxplots.\n",
    "Boxplots for each numerical feature to identify and analyze outliers. Calculate and display statistics about potential outliers. This can be done by calculating the IQR and then using that to identify the lower and upper bounds of the outliers.\n",
    "The label is categorical so not included in outlier detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Subtask 3 - Identify and analyze outliers using visualizations such as boxplots.\n",
    "#\n",
    "# Purpose: Outliers may occur in numerical features. This subtask is to identify and analyse these outliers.\n",
    "#          Boxplots are used to identify and analyse outliers. The label is categorical so not included in outlier detection.\n",
    "# Takeaway: A boxplot per numerical feature is created and displayed. This allows for the manual inspection of the outliers.\n",
    "#          Outliers in the plots are identified as points that are outside of the whiskers of the boxplot. It can be seen from the plots\n",
    "#          that there are outliers in ever feature, but either above or below the upper or lower whiskers respectively.        \n",
    "########\n",
    "sns.set_palette('viridis') # set colour scheme\n",
    "\n",
    "# Get numerical features from the dataset (only numerical features are considered for outlier detection)\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Create boxplots for each numerical feature\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(3, 3, i+1)  # Adjust grid based on number of features\n",
    "    sns.boxplot(y=df[feature])\n",
    "    plt.title(f'Boxplot of {feature}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Boxplots for Numerical Features to Identify Outliers', fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 4: Visualise feature distributions with histograms and KDE plots to understand the overall distribution of each feature.\n",
    "\n",
    "Seaborn has differing functions for histograms and KDE plots. Use these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Subtask 4: Visualise feature distributions with histograms and KDE plots to understand the overall distribution of each feature.\n",
    "#\n",
    "# Purpose: To visualise the distribution of each feature in the dataset. Histograms and KDE plots are used to visualise the distribution of each feature.\n",
    "# Takeaway: A histogram and KDE plot per numerical feature was created and displayed. It showed that ever numerical feature is skewed. The skew direction for each feature is:\n",
    "#           - Area: Right\n",
    "#           - MajorAxisLength: Right\n",
    "#           - MinorAxisLength: Right\n",
    "#           - Eccentricity: Left\n",
    "#           - ConvexArea: Right\n",
    "#           - Extent: Left\n",
    "#           - Perimeter: Right\n",
    "########\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns # list of numerical features\n",
    "\n",
    "# Display histograms for each numerical feature\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(3, 3, i+1)  # Adjust grid based on number of features\n",
    "    sns.histplot(df[feature])\n",
    "    plt.title(f'Histogram of {feature}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Feature Distributions with Histograms', fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "# Create KDE plots for each numerical feature\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Get numerical features from the dataset\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Create KDE plots for each numerical feature\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    plt.subplot(3, 3, i+1)  # Adjust grid based on number of features\n",
    "    sns.kdeplot(df[feature], fill=True)\n",
    "    plt.title(f'KDE Plot of {feature}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Feature Distributions with KDE Plots', fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features are skewed to either the left or right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 5 - Review feature statistics (e.g., mean, standard deviation) to get insights into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Subtask 5 - Review feature statistics (e.g., mean, standard deviation) to get insights into the data.\n",
    "#\n",
    "# Purpose: To review the statistics of the numerical features in the dataset.\n",
    "# Takeaway: From the descriptive statistics it can be observed that:\n",
    "#           - Area: Right skewed as mean is greater than the median (50% percentile)\n",
    "#           - MajorAxisLength: Right skewed as mean is greater than the median (50% percentile)\n",
    "#           - MinorAxisLength: Right skewed as mean is greater than the median (50% percentile)\n",
    "#           - Eccentricity: Left skewed as mean is less than the median (50% percentile)\n",
    "#           - ConvexArea: Right skewed as mean is greater than the median (50% percentile)\n",
    "#           - Extent: Left skewed as mean is less than the median (50% percentile)\n",
    "#           - Perimeter: Right skewed as mean is greater than the median (50% percentile)\n",
    "#           Standard deviations are quite large for Area, ConvexArea, MajorAxisLength, MinorAxisLength, Perimeter. This indicates there are a large range of objects in the dataset\n",
    "########\n",
    "\n",
    "print(\"Basic Statistics for Numerical Features via Pandas Dataframe describe:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Calculate additional statistics that aren't in describe()\n",
    "print(\"\\nAdditional Statistics:\")\n",
    "numerical_stats = pd.DataFrame({\n",
    "    'Median': df.select_dtypes(include=[np.number]).median(),\n",
    "    'Skewness': df.select_dtypes(include=[np.number]).skew(),\n",
    "    'Kurtosis': df.select_dtypes(include=[np.number]).kurt(),\n",
    "    'IQR': df.select_dtypes(include=[np.number]).quantile(0.75) - df.select_dtypes(include=[np.number]).quantile(0.25),\n",
    "    'Range': df.select_dtypes(include=[np.number]).max() - df.select_dtypes(include=[np.number]).min()\n",
    "})\n",
    "display(numerical_stats)\n",
    "\n",
    "# Generate a correlation matrix\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "display(correlation_matrix)\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that lengths and areas are highly correlated, which is expected as area is a function of length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 6 - Normalize or standardize the dataset so that all features contribute equally in distance calculations, which is crucial for clustering.\n",
    "\n",
    "For every numeric feature, we will normalize it to a range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Subtask 6 - Normalize or standardize the dataset so that all features contribute equally in distance calculations, which is crucial for clustering.\n",
    "#\n",
    "# Purpose: Normalise the dataset so that all features contribute equally in distance calculations.\n",
    "# Takeaway: The dataset was normalized to a range of 0 to 1 for every numeric feature.\n",
    "########\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns.tolist() # create list of numerical columns\n",
    "scaler = MinMaxScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns]) # fit then transform the numerical columns\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Impact of the Number of Clusters on KMeans Clustering with Euclidean Distance\n",
    "\n",
    "The subtask for this are:\n",
    "1. Apply KMeans clustering (using Euclidean distance) on the standardized dataset.\n",
    "2. For a range of cluster numbers (e.g., from 1 to 10), compute the inertia (SSE) and plot\n",
    "these values to identify the “elbow” point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Task 2 - Impact of the Number of Clusters on KMeans Clustering with Euclidean Distance\n",
    "#\n",
    "# Purpose: Apply KMeans clustering on the normalised dataset and to determine the optimal number of cluster using the elbow method.\n",
    "# Takeaway: The elbow appears to be when the cluster number is 3. The KElbowVisualizer is a handy library that displays the elbow point.\n",
    "########\n",
    "\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(\n",
    "    model, k=(1,11), metric='distortion', timings=False\n",
    ") #distortion same as Euclidean distance\n",
    "\n",
    "visualizer.fit(df[numerical_columns])        # Fit the data to the visualizer\n",
    "visualizer.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, the elbow appears to be when the cluster number is 5 as after that point the inertia decreases at a slower rate than for lower cluster numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Evaluating the Stability of KMeans and KMeans++ Initialization\n",
    "\n",
    "Subtasks are:\n",
    "1. Run KMeans clustering 50 times using two initialization methods:\n",
    "    - Standard random initialization.\n",
    "    - KMeans++ initialization.\n",
    "2. Compute and compare the average inertia (SSE) and the Silhouette Score for each\n",
    "method over these iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Task 3 - Evaluating the Stability of KMeans and KMeans++ Initialization\n",
    "#\n",
    "# Purpose: To compare the KMeans and Kmeans++ with regard to their stability when initialised randomly in 50 different runs.\n",
    "# Takeaway: KMeans++ is slightly more susceptible to differences in initialisation values. \n",
    "#           This can be observed from the distribution of the inertia and silhouette scores with KMeans++ distribution being more dispersed.\n",
    "##########\n",
    "\n",
    "n_iterations = 50\n",
    "n_clusters = 3  # Using 5 clusters based on the elbow method from previous task\n",
    "random_inertias = []\n",
    "random_silhouette_scores = []\n",
    "kmeans_plus_inertias = []\n",
    "kmeans_plus_silhouette_scores = []\n",
    "\n",
    "for i in range(n_iterations): # iterate 50 times\n",
    "\n",
    "    # random init kmeans\n",
    "    kmeans_random = KMeans(n_clusters=n_clusters, init='random', random_state=i)\n",
    "    kmeans_random.fit(df[numerical_columns])\n",
    "    random_inertias.append(kmeans_random.inertia_)\n",
    "    \n",
    "    # random init silhouette score\n",
    "    labels_random = kmeans_random.labels_\n",
    "    random_silhouette_scores.append(silhouette_score(df[numerical_columns], labels_random))\n",
    "    \n",
    "    # KMeans++ initialisation\n",
    "    kmeans_plus = KMeans(n_clusters=n_clusters, init='k-means++', random_state=i)\n",
    "    kmeans_plus.fit(df[numerical_columns])\n",
    "    kmeans_plus_inertias.append(kmeans_plus.inertia_)\n",
    "    \n",
    "    # calc silhouette score for kmeans++\n",
    "    labels_plus = kmeans_plus.labels_\n",
    "    kmeans_plus_silhouette_scores.append(silhouette_score(df[numerical_columns], labels_plus))\n",
    "\n",
    "# calc average metrics\n",
    "avg_random_inertia = np.mean(random_inertias)\n",
    "avg_random_silhouette = np.mean(random_silhouette_scores)\n",
    "avg_kmeans_plus_inertia = np.mean(kmeans_plus_inertias)\n",
    "avg_kmeans_plus_silhouette = np.mean(kmeans_plus_silhouette_scores)\n",
    "\n",
    "# calc standard deviations \n",
    "std_random_inertia = np.std(random_inertias)\n",
    "std_random_silhouette = np.std(random_silhouette_scores)\n",
    "std_kmeans_plus_inertia = np.std(kmeans_plus_inertias)\n",
    "std_kmeans_plus_silhouette = np.std(kmeans_plus_silhouette_scores)\n",
    "\n",
    "# Display results\n",
    "print(\"Standard Random Initialisation:\")\n",
    "print(f\"Average Inertia: {avg_random_inertia:.2f} (±{std_random_inertia:.2f})\")\n",
    "print(f\"Average Silhouette Score: {avg_random_silhouette:.4f} (±{std_random_silhouette:.4f})\")\n",
    "print(\"\\nKMeans++ Initialisation:\")\n",
    "print(f\"Average Inertia: {avg_kmeans_plus_inertia:.2f} (±{std_kmeans_plus_inertia:.2f})\")\n",
    "print(f\"Average Silhouette Score: {avg_kmeans_plus_silhouette:.4f} (±{std_kmeans_plus_silhouette:.4f})\")\n",
    "\n",
    "# Plot the distribution of inertias for both methods\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(random_inertias, alpha=0.7, label='Random Init')\n",
    "plt.hist(kmeans_plus_inertias, alpha=0.7, label='KMeans++ Init')\n",
    "plt.xlabel('Inertia')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Inertia Values')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(random_silhouette_scores, alpha=0.7, label='Random Init')\n",
    "plt.hist(kmeans_plus_silhouette_scores, alpha=0.7, label='KMeans++ Init')\n",
    "plt.xlabel('Silhouette Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Silhouette Scores')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above, kmeans++ is slightly more susceptible to differences in initialisation values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 4 - Clustering Evaluation Using Purity and Mutual Information\n",
    "\n",
    "Subtasks are:\n",
    "\n",
    "1. Use KMeans (with the optimal k from Question 2) to cluster the data. Assume the\n",
    "dataset contains a ground-truth label column (e.g., \"label\"). For each cluster, assign a\n",
    "label based on the majority class.\n",
    "2. Evaluation Metrics: Compute and report the following:\n",
    "    1. Purity Score: Measures how homogeneous each cluster is relative to the true\n",
    "labels.\n",
    "    2. Mutual Information Score: Quantifies the mutual dependence between the\n",
    "clustering results and the true labels.\n",
    "    3. Silhouette Score: Evaluates the clustering quality without reference to the\n",
    "ground truth by comparing intra-cluster cohesion versus inter-cluster\n",
    "separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Task 4 - Clustering Evaluation Using Purity and Mutual Information\n",
    "#\n",
    "# Purpose: To evaluate the clustering results using purity, mutual information and silhouette score.\n",
    "# Takeaway: The purity score of 0.84 is quite high, indicating that the members of each cluster are quite homogeneous and in agreement with the true labels.\n",
    "#           The mutual information score of 0.3343 is not very high indicating that the structure of the clusters is not very similar to the true labels.\n",
    "#           The silhouette score of 0.3372 is not very high indicating that the clusters are not very well separated. \n",
    "#           Although the cluster purity was high, the clustering results were not very good most likely due to splitting of the Besni class into two clusters.\n",
    "#########\n",
    "\n",
    "\n",
    "# 3 was the optimal k from task 2\n",
    "k = 3\n",
    "\n",
    "# get labels and features\n",
    "X = df.drop('label', axis=1)\n",
    "true_labels = df['label']\n",
    "\n",
    "# KMeans with the optimal k\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, init='k-means++')\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# based on majority class assign a label to each cluster\n",
    "cluster_to_label = {}\n",
    "for cluster_id in range(k):\n",
    "    cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
    "    cluster_true_labels = true_labels.iloc[cluster_indices]\n",
    "    # get the most common label\n",
    "    most_common_label = Counter(cluster_true_labels).most_common(1)[0][0]\n",
    "    cluster_to_label[cluster_id] = most_common_label\n",
    "\n",
    "# print mappings\n",
    "print(\"Cluster to Label Mapping:\")\n",
    "for cluster_id, label in cluster_to_label.items():\n",
    "    print(f\"Cluster {cluster_id} has label: {label}\")\n",
    "\n",
    "# calc purity score\n",
    "def purity_score(y_true, y_pred):\n",
    "    contingency_matrix = np.zeros((k, len(np.unique(y_true))))\n",
    "    \n",
    "    for i in range(len(y_true)):\n",
    "        true_label_idx = np.where(np.unique(y_true) == y_true.iloc[i])[0][0]\n",
    "        contingency_matrix[y_pred[i], true_label_idx] += 1\n",
    "    \n",
    "    return np.sum(np.max(contingency_matrix, axis=1)) / len(y_true)\n",
    "\n",
    "# calc evaluation metrics\n",
    "purity = purity_score(true_labels, cluster_labels)\n",
    "mutual_info = normalized_mutual_info_score(true_labels, cluster_labels)\n",
    "silhouette = silhouette_score(X, cluster_labels)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Purity Score: {purity:.4f}\")\n",
    "print(f\"Normalized Mutual Information Score: {mutual_info:.4f}\")\n",
    "print(f\"Silhouette Score: {silhouette:.4f}\")\n",
    "\n",
    "# Graph clusters\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for cluster_id in range(k):\n",
    "    cluster_points = X[cluster_labels == cluster_id]\n",
    "    plt.scatter(cluster_points.iloc[:, 0], cluster_points.iloc[:, 1], \n",
    "                label=f\"Cluster {cluster_id}: {cluster_to_label[cluster_id]}\")\n",
    "\n",
    "plt.title('Clusters by KMeans')\n",
    "plt.xlabel(X.columns[0])\n",
    "plt.ylabel(X.columns[1])\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for label in np.unique(true_labels):\n",
    "    label_points = X[true_labels == label]\n",
    "    plt.scatter(label_points.iloc[:, 0], label_points.iloc[:, 1], label=label)\n",
    "\n",
    "plt.title('True Labels')\n",
    "plt.xlabel(X.columns[0])\n",
    "plt.ylabel(X.columns[1])\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 Principal Component Analysis (PCA) for Dimensionality Reduction\n",
    "\n",
    "Subtasks are:\n",
    "1. Apply PCA to reduce the dataset to 4 principal components.\n",
    "2. Plot the cumulative variance explained by the principal components and determine\n",
    "how many components are needed to retain 90% of the total variance.\n",
    "3. Create a 3D scatter plot of the first three principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# Task 5 - Principal Component Analysis (PCA) for Dimensionality Reduction\n",
    "#\n",
    "# Purpose: To reduce the dataset to 4 principal components, to plot the cumulative variance explained by the principal components and to determine the number of components needed to retain 90% of the total variance.\n",
    "# Takeaway: PCA was used to reduce the dataset to 4 principal components. From this it was determined that PC1, PC2 and PC3 were needed to retain 90% of the total variance. (3 components were needed). The amount of variance explained by each component is as follows:\n",
    "#           PC1: 0.6903 (0.6903 cumulative)\n",
    "#           PC2: 0.2076 (0.8979 cumulative)\n",
    "#           PC3: 0.0898 (0.9877 cumulative)\n",
    "#           PC4: 0.0081 (0.9958 cumulative)\n",
    "######\n",
    "\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Subtask 1\n",
    "\n",
    "# Apply PCA with 4 components\n",
    "pca = PCA(n_components=4)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "\n",
    "pca_df = pd.DataFrame(\n",
    "    data=X_pca,\n",
    "    columns=['PC1', 'PC2', 'PC3', 'PC4']\n",
    ")\n",
    "\n",
    "pca_df['label'] = true_labels\n",
    "\n",
    "\n",
    "\n",
    "# Subtask 2 - Graph the explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, label='Individual explained variance')\n",
    "plt.step(range(1, len(cumulative_variance) + 1), cumulative_variance, where='mid', label='Cumulative explained variance')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='90% threshold')\n",
    "\n",
    "# Calc how many components needed for 90% variance\n",
    "components_needed = np.argmax(cumulative_variance >= 0.9) + 1\n",
    "plt.axvline(x=components_needed, color='g', linestyle='--', \n",
    "            label=f'{components_needed} components needed for 90% variance')\n",
    "\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nExplained variance ratio by component:\")\n",
    "for i, var in enumerate(explained_variance):\n",
    "    print(f\"PC{i+1}: {var:.4f} ({cumulative_variance[i]:.4f} cumulative)\")\n",
    "\n",
    "print(f\"\\nNumber of components needed to retain 90% variance: {components_needed}\")\n",
    "\n",
    "# Make a 3D scatter plot of the first three principal components\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "unique_labels = np.unique(true_labels)\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    mask = pca_df['label'] == label\n",
    "    ax.scatter(\n",
    "        pca_df.loc[mask, 'PC1'],\n",
    "        pca_df.loc[mask, 'PC2'],\n",
    "        pca_df.loc[mask, 'PC3'],\n",
    "        label=label,\n",
    "        color=color,\n",
    "        alpha=0.7,\n",
    "        s=50\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "ax.set_title('3D Projection of First Three Principal Components')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6 - Density Based Clustering Using DBSCAN with Different Distance Metrics\n",
    "\n",
    "Subtasks are:\n",
    "\n",
    "1. Apply DBSCAN to the dataset twice:\n",
    "    1. Once using Euclidean distance.\n",
    "    2. Once using Mahalanobis distance.\n",
    "2. Determine the optimal values for eps (ε) and min_samples for each distance metric.\n",
    "3. Compare the clustering results from both distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######\n",
    "# Task 6 - Density Based Clustering Using DBSCAN with Different Distance Metrics\n",
    "#\n",
    "# Purpose: To apply DBSCAN to the dataset twice: once using Euclidean distance and once using Mahalanobis distance.\n",
    "# Takeaway: Care had to be taken when using DBSCAN as the number of clusters to be used cannot be set beforehand. As we know there are two label ideally we would have two clusters. \n",
    "#            DBSCAN also assigns a -1 label to noise points which had to be filtered out when calculating the silhouette score.\n",
    "#            The best euclidean parameters were eps=0.65, min_samples=5. The best mahalanobis parameters were eps=7.00, min_samples=5.\n",
    "#            The cluster evaluation metrics were as follows:\n",
    "#           Euclidean - Purity: 0.5522, Mutual Information: 0.0148, Silhouette: 0.2529876685755327, Cluster Sizes: 3\n",
    "#           Mahalanobis - Purity: 0.5022, Mutual Information: 0.0009, Silhouette: 0.6096798484416615, Cluster Sizes: 3\n",
    "#           The Euclidean distance had a better clustering result as it had a higher purity and mutual information score. The mahalanobis distance had a higher silhouette score which indicated that the clusters were more separated.\n",
    "#######\n",
    "\n",
    "# Calc purity\n",
    "def calculate_purity(true_labels, predicted_labels):\n",
    "    contingency_matrix = np.zeros((len(np.unique(true_labels)), len(np.unique(predicted_labels))))\n",
    "    \n",
    "    for i, true_label in enumerate(np.unique(true_labels)):\n",
    "        for j, pred_label in enumerate(np.unique(predicted_labels)):\n",
    "            contingency_matrix[i, j] = np.sum((true_labels == true_label) & (predicted_labels == pred_label))\n",
    "    \n",
    "    cluster_sizes = np.sum(contingency_matrix, axis=0)\n",
    "    max_correct = np.sum(np.max(contingency_matrix, axis=0))\n",
    "    \n",
    "    purity = max_correct / len(true_labels)\n",
    "    return purity\n",
    "\n",
    "\n",
    "\n",
    "# find optimal eps for euclidean distance\n",
    "def find_optimal_eps(X, min_samples, target_clusters=2, eps_range=np.arange(0.1, 2.0, 0.05)):\n",
    "    results = []\n",
    "    \n",
    "    for eps in eps_range:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(X)\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        noise_points = np.sum(labels == -1)\n",
    "        \n",
    "        results.append({\n",
    "            'eps': eps,\n",
    "            'n_clusters': n_clusters,\n",
    "            'noise_points': noise_points,\n",
    "            'labels': labels\n",
    "        })\n",
    "    \n",
    "    return min(results, key=lambda x: abs(x['n_clusters'] - target_clusters))\n",
    "\n",
    "# min samples to try\n",
    "min_samples_values = [5, 10, 15, 20]\n",
    "euclidean_results = []\n",
    "\n",
    "for min_samples in min_samples_values:\n",
    "    result = find_optimal_eps(X_std, min_samples)\n",
    "    result['min_samples'] = min_samples\n",
    "    euclidean_results.append(result)\n",
    "    print(f\"Euclidean - min_samples={min_samples}, eps={result['eps']:.2f}, clusters={result['n_clusters']}, noise points={result['noise_points']}\")\n",
    "\n",
    "# choose best eps\n",
    "best_euclidean = min(euclidean_results, key=lambda x: abs(x['n_clusters'] - 2))\n",
    "print(f\"\\nBest Euclidean parameters: eps={best_euclidean['eps']:.2f}, min_samples={best_euclidean['min_samples']}\")\n",
    "\n",
    "# distances for mahalanobis\n",
    "cov = np.cov(X_std.T)\n",
    "inv_cov = np.linalg.inv(cov)\n",
    "\n",
    "#  custom distance matrix using Mahalanobis distance\n",
    "def create_mahalanobis_distance_matrix(X, inv_cov):\n",
    "    n = X.shape[0]\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            dist = mahalanobis(X[i], X[j], inv_cov)\n",
    "            dist_matrix[i, j] = dist\n",
    "            dist_matrix[j, i] = dist\n",
    "    \n",
    "    return dist_matrix\n",
    "\n",
    "# create Mahalanobis distance matrix\n",
    "mahalanobis_dist_matrix = create_mahalanobis_distance_matrix(X_std, inv_cov)\n",
    "\n",
    "# find optimal eps for Mahalanobis distance\n",
    "def find_optimal_eps_precomputed(dist_matrix, min_samples, target_clusters=2, eps_range=np.arange(0.5, 10.0, 0.5)):\n",
    "    results = []\n",
    "    \n",
    "    for eps in eps_range:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "        labels = dbscan.fit_predict(dist_matrix)\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        noise_points = np.sum(labels == -1)\n",
    "        \n",
    "        results.append({\n",
    "            'eps': eps,\n",
    "            'n_clusters': n_clusters,\n",
    "            'noise_points': noise_points,\n",
    "            'labels': labels\n",
    "        })\n",
    "    \n",
    "    return min(results, key=lambda x: abs(x['n_clusters'] - target_clusters))\n",
    "\n",
    "mahalanobis_results = []\n",
    "\n",
    "for min_samples in min_samples_values:\n",
    "    result = find_optimal_eps_precomputed(mahalanobis_dist_matrix, min_samples)\n",
    "    result['min_samples'] = min_samples\n",
    "    mahalanobis_results.append(result)\n",
    "    print(f\"Mahalanobis - min_samples={min_samples}, eps={result['eps']:.2f}, clusters={result['n_clusters']}, noise points={result['noise_points']}\")\n",
    "\n",
    "# best parameters for mahalanobis\n",
    "best_mahalanobis = min(mahalanobis_results, key=lambda x: abs(x['n_clusters'] - 2))\n",
    "print(f\"\\nBest Mahalanobis parameters: eps={best_mahalanobis['eps']:.2f}, min_samples={best_mahalanobis['min_samples']}\")\n",
    "\n",
    "# dbscan with best parameters for euclidean\n",
    "euclidean_dbscan = DBSCAN(eps=best_euclidean['eps'], min_samples=best_euclidean['min_samples'])\n",
    "euclidean_labels = euclidean_dbscan.fit_predict(X_std)\n",
    "\n",
    "# dbscan with best parameters for mahalanobis\n",
    "mahalanobis_dbscan = DBSCAN(eps=best_mahalanobis['eps'], min_samples=best_mahalanobis['min_samples'], metric='precomputed')\n",
    "mahalanobis_labels = mahalanobis_dbscan.fit_predict(mahalanobis_dist_matrix)\n",
    "\n",
    "# metrics for Euclidean distance\n",
    "euclidean_purity = calculate_purity(true_labels, euclidean_labels)\n",
    "euclidean_mi = mutual_info_score(true_labels, euclidean_labels)\n",
    "\n",
    "# filter out noise cluster (-1 label)\n",
    "euclidean_filtered_indices = euclidean_labels != -1\n",
    "euclidean_filtered_labels = euclidean_labels[euclidean_filtered_indices]\n",
    "true_labels_euclidean_filtered = true_labels[euclidean_filtered_indices]\n",
    "\n",
    "# silhouette score\n",
    "if len(set(euclidean_filtered_labels)) > 1:\n",
    "    euclidean_silhouette = silhouette_score(X_std[euclidean_filtered_indices], euclidean_filtered_labels)\n",
    "else:\n",
    "    euclidean_silhouette = \"N/A\"  # Not applicable if only one cluster remains\n",
    "\n",
    "# metrics for Mahalanobis distance\n",
    "mahalanobis_purity = calculate_purity(true_labels, mahalanobis_labels)\n",
    "mahalanobis_mi = mutual_info_score(true_labels, mahalanobis_labels)\n",
    "\n",
    "# filter out noise cluster (-1 label)\n",
    "mahalanobis_filtered_indices = mahalanobis_labels != -1\n",
    "mahalanobis_filtered_labels = mahalanobis_labels[mahalanobis_filtered_indices]\n",
    "true_labels_mahalanobis_filtered = true_labels[mahalanobis_filtered_indices]\n",
    "\n",
    "if len(set(mahalanobis_filtered_labels)) > 1:\n",
    "    mahalanobis_silhouette = silhouette_score(X_std[mahalanobis_filtered_indices], mahalanobis_filtered_labels)\n",
    "else:\n",
    "    mahalanobis_silhouette = \"N/A\"  # Not applicable if only one cluster remains\n",
    "\n",
    "euclidean_cluster_size = len(set(euclidean_labels))\n",
    "mahalanobis_cluster_size = len(set(mahalanobis_labels))\n",
    "\n",
    "print(\"\\nMetrics results:\")\n",
    "print(f\"Euclidean - Purity: {euclidean_purity:.4f}, Mutual Information: {euclidean_mi:.4f}, Silhouette: {euclidean_silhouette}, Cluster Sizes: {euclidean_cluster_size}\")\n",
    "print(f\"Mahalanobis - Purity: {mahalanobis_purity:.4f}, Mutual Information: {mahalanobis_mi:.4f}, Silhouette: {mahalanobis_silhouette}, Cluster Sizes: {mahalanobis_cluster_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7 - Clustering Performance on PCA-Reduced v Full Dataset\n",
    "\n",
    "1. Apply KMeans clustering to:\n",
    "    1. The original standardized dataset.\n",
    "    2. The PCA-transformed dataset (using the principal components from Question5).\n",
    "2.  Evaluate the clustering quality using the Silhouette Score.\n",
    "3. Compare whether the PCA-transformed dataset results in better-separated and more compact clusters relative to the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Task 7 - Clustering Performance on PCA-Reduced v Full Dataset\n",
    "#\n",
    "# Purpose: To apply KMeans clustering to the original and PCA-transformed datasets and to evaluate the clustering quality using the Silhouette Score.\n",
    "# Takeaway: The results for the two datasets were:\n",
    "#           Original Dataset: k=2, Silhouette Score=0.441\n",
    "#PCA Dataset: k=2, Silhouette Score=0.484\n",
    "#########\n",
    "\n",
    "\n",
    "# function to run kmeans and eval silhouette and inertia score\n",
    "def apply_kmeans_and_evaluate(data, name, n_clusters_range=range(2, 11)):\n",
    "    results = []\n",
    "    \n",
    "    for k in n_clusters_range:\n",
    "        # Apply KMeans\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(data)\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        sil_score = silhouette_score(data, cluster_labels)\n",
    "        \n",
    "        # Calculate inertia (sum of squared distances to closest centroid)\n",
    "        inertia = kmeans.inertia_\n",
    "        \n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'silhouette': sil_score,\n",
    "            'inertia': inertia,\n",
    "            'labels': cluster_labels\n",
    "        })\n",
    "        \n",
    "        print(f\"{name} with {k} clusters - Silhouette Score: {sil_score:.3f}, Inertia: {inertia:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# original dataset\n",
    "original_results = apply_kmeans_and_evaluate(X_std, \"Original Dataset\")\n",
    "\n",
    "# pca dataset\n",
    "pca_results = apply_kmeans_and_evaluate(X_pca, \"PCA Dataset\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([r['k'] for r in original_results], [r['silhouette'] for r in original_results], 'o-', label='Original Dataset')\n",
    "plt.plot([r['k'] for r in pca_results], [r['silhouette'] for r in pca_results], 'o-', label='PCA Dataset')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "# find the best on silhouette score\n",
    "best_k_original = max(original_results, key=lambda x: x['silhouette'])\n",
    "best_k_pca = max(pca_results, key=lambda x: x['silhouette'])\n",
    "\n",
    "print(\"\\nBest results:\")\n",
    "print(f\"Original Dataset: k={best_k_original['k']}, Silhouette Score={best_k_original['silhouette']:.3f}\")\n",
    "print(f\"PCA Dataset: k={best_k_pca['k']}, Silhouette Score={best_k_pca['silhouette']:.3f}\")\n",
    "\n",
    "# determine which is better\n",
    "if best_k_pca['silhouette'] > best_k_original['silhouette']:\n",
    "    print(\"\\nPCA dataset is better.\")\n",
    "else:\n",
    "    print(\"\\nOriginal dataset is better.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset's best silhouette score is 0.441. The PCA dataset's best silhoutte score is 0.442. Therefore the PCA dataset results in a better separated and more compact clusters, however the difference between the results is very minor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8 - Clustering Using t-SNE\n",
    "\n",
    "1. Apply t-SNE (using the exact method) to reduce the dataset to 4 components.\n",
    "2. Create a 3D scatter plot of the first three t-SNE components.\n",
    "3. Apply KMeans clustering on the t‐SNE–reduced data using an appropriate number of\n",
    "clusters (e.g., based on prior optimal k or an elbow method on the t‐SNE output).\n",
    "4. Evaluate the clustering performance on the t‐SNE–reduced data using metrics such\n",
    "as the Silhouette Score and compare these results to clustering on the original and\n",
    "PCA–transformed dataset.\n",
    "5. Discuss whether the clusters formed on the t-SNE–reduced data are more distinct\n",
    "and how well they correspond to the known data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Task 8 - Clustering Using t-SNE\n",
    "#\n",
    "# Purpose: To apply t-SNE to generate a new dataset with 4 components. Compare the clustering results using KMeans on the t-SNE reduced data, the original data and the PCA transformed data.\n",
    "# Takeaway: The results for the three datasets were:\n",
    "#           Original Dataset: k=2, Silhouette Score=0.441\n",
    "#           PCA Dataset: k=2, Silhouette Score=0.484\n",
    "#           t-SNE Dataset: k=2, Silhouette Score=0.400\n",
    "#           The PCA dataset has the highest silhouette score, therefore it has the best defined clusters.\n",
    "#           The t-SNE dataset has the lowest silhouette score, therefore it has the least well defined cluster.\n",
    "#           Even though t-SNE is a dimensionality reduction technique which theoretically could improve cluster cohesion, the original dataset has only 8 features and therefore may not possess enough dimensions to benefit from this technique.\n",
    "#########\n",
    "\n",
    "\n",
    "# compute t-SNE \n",
    "tsne = TSNE(n_components=4, method='exact', random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_std)\n",
    "\n",
    "# 3D scatter plot of the first three components\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "if 'label' in df.columns:\n",
    "    labels = df['label'].unique()\n",
    "    for i, label in enumerate(labels):\n",
    "        indices = df['label'] == label\n",
    "        ax.scatter(X_tsne[indices, 0], X_tsne[indices, 1], X_tsne[indices, 2], \n",
    "                   label=label, alpha=0.7)\n",
    "else:\n",
    "    ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2], alpha=0.7)\n",
    "\n",
    "ax.set_title('3D t-SNE Visualisation')\n",
    "ax.set_xlabel('t-SNE Component 1')\n",
    "ax.set_ylabel('t-SNE Component 2')\n",
    "ax.set_zlabel('t-SNE Component 3')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Apply KMeans clustering on the t-SNE-reduced data\n",
    "# First, determine the optimal number of clusters using the elbow method\n",
    "distortions = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_tsne)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "    \n",
    "    labels = kmeans.labels_\n",
    "    silhouette_avg = silhouette_score(X_tsne, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "\n",
    "# calc silhouette scores\n",
    "tsne_results = []\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_tsne)\n",
    "    silhouette_avg = silhouette_score(X_tsne, labels)\n",
    "    tsne_results.append({'k': k, 'silhouette': silhouette_avg})\n",
    "\n",
    "best_k_tsne = max(tsne_results, key=lambda x: x['silhouette'])\n",
    "\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Original Dataset: k={best_k_original['k']}, Silhouette Score={best_k_original['silhouette']:.3f}\")\n",
    "print(f\"PCA Dataset: k={best_k_pca['k']}, Silhouette Score={best_k_pca['silhouette']:.3f}\")\n",
    "print(f\"t-SNE Dataset: k={best_k_tsne['k']}, Silhouette Score={best_k_tsne['silhouette']:.3f}\")\n",
    "\n",
    "# find the best\n",
    "best_method = max([\n",
    "    ('Original', best_k_original['silhouette']),\n",
    "    ('PCA', best_k_pca['silhouette']),\n",
    "    ('t-SNE', best_k_tsne['silhouette'])\n",
    "], key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nThe {best_method[0]} dataset provides better clustering quality with a silhouette score of {best_method[1]:.3f}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-SNE dataset has the lowest silhouette score, therefore it has the least well defined cluster. t-SNE (t-Distributed Stochastic Neighbour Embedding). Even though t-SNE is a dimensionality reduction technique which theoretically could improve cluster cohesion, the original dataset has only 8 features and therefore may not possess enough dimensions to benefit from this technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
