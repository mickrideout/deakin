{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Heart Disease Prediction Model\n",
    "\n",
    " In this notebook, we attempt to replicate the results of the paper \"An efficient stacking-based ensemble technique for early heart attack prediction\". We then implement an improved solution in an attempt to increase the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Part 1 - Academic Paper Reproduction\n",
    "\n",
    " In this part, we attempt to replicate the results of the paper \"An efficient stacking-based ensemble technique for early heart attack prediction\". We do this in two sections, data preprocessing and model architecture.\n",
    "\n",
    " It is a fundamental tennant of science that findings should be reproducible. Difficulties in reproducing the paper's results will be discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Preprocessing\n",
    "\n",
    " This sections loads the dataset and performs data clearning, transformation and feature selection all with the aim of reproducing the paper's results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### DataSet Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('framingham.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Missing Values\n",
    "\n",
    " Here we replicate the paper's approach to handling missing values. We perform explicity imputation as stated in the paper,\n",
    "\n",
    " and also impute the missing values for numerical columns with median and categorical columns with mode.\n",
    "\n",
    " Non feature selected columns are to be dropped anyhow so the superflous imputations for them is not a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing values before preprocessing:\")\n",
    "print(df.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Glucose was explicitly mentioned a having mode of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['glucose'] = df['glucose'].fillna(df['glucose'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Although not stated, impute the missing values for numerical columns with median and categorical columns with mode.\n",
    "\n",
    " The logics is that columns not used will be dropped anyhow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numerical_columns:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "print(\"Numeric columns which had median imputation:\")\n",
    "print(df.select_dtypes(include=['float64', 'int64']).columns)\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Takeaways are to alway explicity state what the imputations are and to what columns they are applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Outlier Removal\n",
    "\n",
    " Here we replicate the paper's approach to outlier removal. The columns totChol and sysBP are mentioned to have outliers.\n",
    "\n",
    " We remove the outliers for these columns using the IQR (Interquartile Range) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier removal for totChol and sysBP\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "df = remove_outliers(df, 'totChol')\n",
    "df = remove_outliers(df, 'sysBP')\n",
    "\n",
    "print(\"\\nDataset shape after outlier removal:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The number of rows removed wasnt stated in the paper. This would have been useful to verify if the outlier removal was equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Data Standardisation\n",
    "\n",
    " Here we replicate the paper's approach to data standardisation. We use the StandardScaler from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = df.drop('TenYearCHD', axis=1)\n",
    "target = df['TenYearCHD']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "features_scaled = pd.DataFrame(features_scaled, columns=features.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The paper provided the formula for standardisation which was useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Class Imbalance Handling\n",
    "\n",
    " Here we replicate the paper's approach to class imbalance handling. We use the SMOTE (Synthetic Minority Over-sampling Technique) from imblearn.\n",
    "\n",
    " Default SMOTE rebalancing produced more records than the paper's approach. So balanced resampling was needed. Default values were\n",
    "\n",
    " used as there was no information on the parameters used by the authors.\n",
    "\n",
    " Random seed of 42 is used to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class imbalance handling using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(features_scaled, target)\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_resampled).value_counts())\n",
    "\n",
    "# Resample to get exactly 1697 records for each class\n",
    "class_0 = X_resampled[y_resampled == 0]\n",
    "class_1 = X_resampled[y_resampled == 1]\n",
    "\n",
    "# Randomly sample 1697 records from each class\n",
    "X_balanced = pd.concat([\n",
    "    class_0.sample(n=1697, random_state=42),\n",
    "    class_1.sample(n=1697, random_state=42)\n",
    "])\n",
    "\n",
    "y_balanced = pd.Series([0] * 1697 + [1] * 1697)\n",
    "\n",
    "print(\"\\nClass distribution after resampling:\")\n",
    "print(pd.Series(y_balanced).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It is important to specify inputs and outputs from data preprocessing steps and the arguments used for algorithms.\n",
    "\n",
    " Resampling of the smote output allowed the number of rows to be reduced to 3394, which is the same as the paper's approach.\n",
    "\n",
    " The random_state for the SMOTE rebalancer would also have been useful to specify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Feature Correlation Analysis\n",
    "\n",
    " Here we replicate the paper's approach to feature correlation analysis. We use the correlation matrix from sklearn.\n",
    "\n",
    " We remove the highly correlated features using the correlation threshold of 0.85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation analysis\n",
    "correlation_matrix = X_resampled.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Remove highly correlated features (correlation > 0.85)\n",
    "high_corr_features = set()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.85:\n",
    "            high_corr_features.add(correlation_matrix.columns[i])\n",
    "\n",
    "X_resampled = X_resampled.drop(columns=list(high_corr_features))\n",
    "print(\"\\nFeatures removed due to high correlation:\", high_corr_features)\n",
    "print(\"Remaining features:\", X_resampled.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The results from this step were the same as the paper's. No features were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Feature Selection\n",
    "\n",
    " Here we replicate the paper's approach to feature selection. We use the RandomForestClassifier from sklearn.\n",
    "\n",
    " We select the top 10 features using the feature importance scores.\n",
    "\n",
    " The paper alluded to the use of random forest for feature selection and to the number of features selected (10)\n",
    "\n",
    " Random seed of 42 is used to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Random Forest feature selection\n",
    "rf_selector = RandomForestClassifier(random_state=42)\n",
    "rf_selector.fit(features, target)\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features.columns,\n",
    "    'importance': rf_selector.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "# Select top 10 features\n",
    "top_10_features = feature_importance.head(10)['feature'].tolist()\n",
    "# X_selected = X_resampled[top_10_features]\n",
    "\n",
    "print(\"Top 10 selected features:\")\n",
    "print(top_10_features)\n",
    "\n",
    "# The assumed features selected by the paper. The 10 ten features in Figure 2 Visualize the features score of random forest features\n",
    "paper_features = [\"age\", \"sysBP\", \"BMI\", \"totChol\", \"diaBP\", \"glucose\", \"heartRate\", \"cigsPerDay\", \"education\", \"prevalentHyp\"]\n",
    "\n",
    "\n",
    "X_selected = X_resampled[paper_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The paper did not state explicity what the features were. So we had to make an assumption that it was the top 10 features\n",
    "\n",
    " based on the figure 2. Based on this, the features determined as important were not the same as the paper's, even though\n",
    "\n",
    " the same method was used (RF). The male column was not included in the paper's features. prevalentHyp was included in the paper's\n",
    "\n",
    " features and not in ours. Again the paper lacked information to replicate feature selection results. The random seed was not specified\n",
    "\n",
    " so the results were not reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Train / Test Split\n",
    "\n",
    " The train / test split ratio was stated in the paper as 70 / 30. However the strategy was not specified.\n",
    "\n",
    " We used random_state=42 to ensure reproducibility for our random splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y_resampled, test_size=0.3, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The strategy was not specified in the paper. If a random splitter was used, the results would not be reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Model Architecture and Evaluation\n",
    "# Here we train and evaluate the models as described in the paper. We use the XGBoost, Random Forest, Decision Tree and KNN models.\n",
    "# We also use the FT-DNN and DNN models. We use the same architecture as the paper's.\n",
    "# The models are evaludated using the accuracy, precision, recall, f1-score and roc-auc score.\n",
    "# Any discrepanncy in the results will be discussed in the results section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model results from the paper\n",
    "paper_results = pd.DataFrame({\n",
    "    'Model': ['RF', 'KNN', 'DT', 'XGB', 'FT-DNN', 'DNN', 'ML_Ensemble', 'MDLSM'],\n",
    "    'Accuracy': [94.02, 93.45, 92.35, 94.03, 80.19, 76.73, 94.1, 94.14],\n",
    "    'Precision': [94.01, 93.53, 92.23, 94.03, 77.03, 72.85, 94.04, 94.25],\n",
    "    'Recall': [94.01, 93.21, 92.22, 94.02, 86.77, 86.19, 94.05, 94.06],\n",
    "    'F1-score': [94.01, 93.25, 91.22, 94.02, 69.43, 67.32, 94.05, 94.06],\n",
    "    'AUC-ROC': [98, 93, 91, 98, 86.2, 83.1, 99, 99]\n",
    "})\n",
    "paper_results.set_index('Model', inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate the models\n",
    "def train_and_evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    results = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-score': f1_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        results['AUC-ROC'] = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    return model, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Baseline ML Models\n",
    "\n",
    " Here we train the baseline ML models as described in the paper. We use the XGBoost, Random Forest, Decision Tree and KNN models.\n",
    "\n",
    " The models are evaludated using the accuracy, precision, recall, f1-score and roc-auc score.\n",
    "\n",
    " Default values were used for the models as there was no information on the parameters used by the authors.\n",
    "\n",
    " Random seed of 42 is used to ensure reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " All models listed in the paper should have specified the hyperparameters used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Deep Learning Models\n",
    "\n",
    " Here we implement the deep learning models from the paper. Those models are FT-DNN and DNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### FT-DNN Architecture\n",
    "\n",
    " The FT-DNN architecture was described a follows:\n",
    "\n",
    " A 4 layer feedforward neural network with 16, 12, 8 and 4 neurons in the layers respectively.\n",
    "\n",
    " ReLU is the activation function.\n",
    "\n",
    " Adam was the optimizer.\n",
    "\n",
    " Binary crossentropy was the loss function.\n",
    "\n",
    " Learning rate of 0.001 was specified.\n",
    "\n",
    " The metrics were not specified. So we used the f1_score metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ft_dnn():\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(12, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(4, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['f1_score']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### DNN Architecture\n",
    "\n",
    " The DNN architecture was described a follows:\n",
    "\n",
    " A 4 layer feedforward neural network with 12, 10, 8 and 6 neurons in the layers respectively.\n",
    "\n",
    " ReLU is the activation function.\n",
    "\n",
    " Adam was the optimizer.\n",
    "\n",
    " Binary crossentropy was the loss function.\n",
    "\n",
    " The metrics were not specified. So we used the f1_score metric.\n",
    "\n",
    " No learning rate was specified so we did not specify one either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn():\n",
    "    model = Sequential([\n",
    "        Dense(12, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(10, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(6, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['f1_score']\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Deep Learning Model Training\n",
    "\n",
    " We have had to use common sense values for the training hyperparameters as none were specified.\n",
    "\n",
    " We will use epochs=400, batch_size=32, validation_split=0.1, verbose=0 as the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DL models\n",
    "ft_dnn = create_ft_dnn()\n",
    "dnn = create_dnn()\n",
    "\n",
    "# Train FT-DNN\n",
    "ft_dnn_history = ft_dnn.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=400,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train DNN\n",
    "dnn_history = dnn.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=400,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate DL models\n",
    "dl_models = {\n",
    "    'FT-DNN': ft_dnn,\n",
    "    'DNN': dnn\n",
    "}\n",
    "\n",
    "dl_results = {}\n",
    "for name, model in dl_models.items():\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    \n",
    "    results = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-score': f1_score(y_test, y_pred),\n",
    "        'AUC-ROC': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    dl_results[name] = results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The paper did not specify the training hyperparameters for the deep learning models. This is crucial information\n",
    "\n",
    " for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Ensemble Models\n",
    "# Here we implement the ensemble models as described in the paper. We use the ML Ensemble and MDLSM models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## ML Ensemble Model\n",
    "\n",
    " The ML Ensemble model is a simple ensemble of the baseline ML models.\n",
    "\n",
    " It aggregates the predictions of the baseline ML models using a simple mean vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLEnsemble:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        return np.mean(predictions, axis=0) > 0.5\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        predictions = np.array([model.predict_proba(X)[:, 1] for model in self.models])\n",
    "        return np.mean(predictions, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## MDLSM Model\n",
    "\n",
    " The MDLSM model is a meta-learning model that uses the ML Ensemble model to predict the class labels and the deep learning models to predict the probabilities.\n",
    "\n",
    " It aggregates the predictions of the ML Ensemble model and the deep learning models using a simple mean vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDLSM:\n",
    "    def __init__(self, ml_ensemble, ft_dnn, dnn):\n",
    "        self.ml_ensemble = ml_ensemble\n",
    "        self.ft_dnn = ft_dnn\n",
    "        self.dnn = dnn\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ml_pred = self.ml_ensemble.predict(X)\n",
    "        ft_dnn_pred = (self.ft_dnn.predict(X).flatten() > 0.5).astype(int)\n",
    "        dnn_pred = (self.dnn.predict(X).flatten() > 0.5).astype(int)\n",
    "        \n",
    "        predictions = np.vstack([ml_pred, ft_dnn_pred, dnn_pred])\n",
    "        return np.mean(predictions, axis=0) > 0.5\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        ml_proba = self.ml_ensemble.predict_proba(X)\n",
    "        ft_dnn_proba = self.ft_dnn.predict(X).flatten()\n",
    "        dnn_proba = self.dnn.predict(X).flatten()\n",
    "        \n",
    "        predictions = np.vstack([ml_proba, ft_dnn_proba, dnn_proba])\n",
    "        return np.mean(predictions, axis=0)\n",
    "\n",
    "# Train ensemble models\n",
    "ml_ensemble = MLEnsemble([\n",
    "    models['DT'],\n",
    "    models['XGB'],\n",
    "    models['KNN'],\n",
    "    models['RF']\n",
    "])\n",
    "ml_ensemble.fit(X_train, y_train)\n",
    "\n",
    "mdlsm = MDLSM(ml_ensemble, ft_dnn, dnn)\n",
    "\n",
    "# Evaluate ensemble models\n",
    "ensemble_models = {\n",
    "    'ML Ensemble': ml_ensemble,\n",
    "    'MDLSM': mdlsm\n",
    "}\n",
    "\n",
    "ensemble_results = {}\n",
    "for name, model in ensemble_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    results = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-score': f1_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        results['AUC-ROC'] = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    ensemble_results[name] = results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all models for evaluation\n",
    "all_models = {\n",
    "    **models,  # Baseline ML models\n",
    "    **dl_models,  # Deep Learning models\n",
    "    **ensemble_models  # Ensemble models\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model Evaluation and Result Verification\n",
    "\n",
    " Here we evaluate the models and compare the results with the paper's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all models for evaluation\n",
    "# Combine all results\n",
    "all_results = {\n",
    "    **baseline_results,\n",
    "    **dl_results,\n",
    "    **ensemble_results\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_results).T\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'AUC-ROC']\n",
    "\n",
    "for metric in metrics:\n",
    "    comparison_df[metric] = (comparison_df[metric] * 100).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(\"\\nComparing our results with paper results:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Our Results': comparison_df[metrics].values.flatten(),\n",
    "    'Paper Results': paper_results[metrics].values.flatten()\n",
    "}, index=pd.MultiIndex.from_product([comparison_df.index, metrics], names=['Model', 'Metric']))\n",
    "comparison['Difference (%)'] = ((comparison['Our Results'] - comparison['Paper Results']) / comparison['Paper Results'] * 100).round(2)\n",
    "print(comparison)\n",
    "\n",
    "comparison.to_csv('model_comparison_results.csv')\n",
    "print(\"\\nResults have been saved to 'model_comparison_results.csv'\")\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "x = np.arange(len(comparison_df.index))\n",
    "width = 0.35\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.bar(x - width/2, comparison_df[metric], width, label='Replication Results')\n",
    "    plt.bar(x + width/2, paper_results[metric], width, label='Paper Results')\n",
    "    \n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel(f'{metric.capitalize()} (%)')\n",
    "    plt.title(f'Comparison of {metric.capitalize()}')\n",
    "    plt.xticks(x, comparison_df.index, rotation=45)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model Comparison\n",
    "\n",
    " We were not able to replicate the results of the paper exactly as there were some differences in the results.\n",
    "\n",
    " The MDSLM model (the final ensemble model) had replication results for f1-score being 83.34 which was 11.4% lower than the paper results of 94.06\n",
    "\n",
    " Most of the replication results show lower performance than the paper results.\n",
    "\n",
    " Some variations, such as higher f1-scores for DNN and FT-DNN and higher AUC-ROC for RF in the replication were observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Replication Conclusion\n",
    "\n",
    " Whilst it was possible to replicate the architecture of the models, the results were not able to be replicated exactly.\n",
    "\n",
    " The inability to replicate the results accurately is mostly attributable to the\n",
    "\n",
    " lack of crucial information in the paper. The main items of information that were missing were:\n",
    "\n",
    " - The hyperparameters used for the models\n",
    "\n",
    " - The training hyperparameters for the deep learning models.\n",
    "\n",
    " - The train / test split ratio and methodology.\n",
    "\n",
    " - Exact row counts for steps in the data cleaning process.\n",
    "\n",
    " - Library versions used for the models.\n",
    "\n",
    " - Random seeds used for the models.\n",
    "\n",
    " - More precise information on the data preprocessing steps used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Part 2 - Heart Disease Prediction Alternate Solution\n",
    "\n",
    " This notebook implements an alternate solution to the heart disease prediction problem.\n",
    "\n",
    " It uses AutoGluon to train a model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import random\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Dataset Loading\n",
    "\n",
    " We reload the dataset to clearn any modifications made in part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('framingham.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Preprocessing\n",
    "\n",
    " This sections loads the dataset and performs data clearning and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Missing Value Imputation\n",
    "\n",
    " Here we impute all numerical columns with median if they have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing values before preprocessing:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Impute missing values for numerical columns with median\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numerical_columns:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df.drop(columns=['currentSmoker'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "for i, column in enumerate(df.columns, 1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    sns.histplot(data=df, x=column, hue='TenYearCHD', multiple=\"stack\")\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Skewness Transformation\n",
    "\n",
    " Here we deal with the right skewness of some numerical columns. Binning is also performed on cigsPerDay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins for cigsPerDay based on custom ranges\n",
    "df['cigsPerDay_binned'] = pd.cut(df['cigsPerDay'], \n",
    "                                bins=[-float('inf'), 0, 10, 20, 30, float('inf')],\n",
    "                                labels=[0, 1, 2, 3, 4])\n",
    "print(\"\\nCigarettes per day distribution after binning:\")\n",
    "print(df['cigsPerDay_binned'].value_counts())\n",
    "df.drop(columns=['cigsPerDay'], inplace=True)\n",
    "\n",
    "# Apply log transformation to right-skewed numerical columns\n",
    "skewed_columns = ['totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\n",
    "\n",
    "for col in skewed_columns:\n",
    "    df[col] = np.log1p(df[col])\n",
    "\n",
    "df[\"glucose\"] = np.log(np.log(df[\"glucose\"])) # it is highly skewed\n",
    "\n",
    "print(\"\\nSkewness after log transformation:\")\n",
    "print(df[skewed_columns].skew())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " After these transforms, the features have a normal distribution and the cigsPerDay column is binned with 5 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('TenYearCHD', axis=1)\n",
    "target = df['TenYearCHD']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Data Standardisation\n",
    "\n",
    " Here we standardise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data standardisation\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "features_scaled = pd.DataFrame(features_scaled, columns=features.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Class Imbalance Handling\n",
    "\n",
    " Here we handle the class imbalance using SMOTE as the target variable is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class imbalance handling using SMOTE\n",
    "print(\"\\nClass distribution before SMOTE:\")\n",
    "print(pd.Series(target).value_counts())\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(features_scaled, target)\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_resampled).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Plot the features histograms to show the affects of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "for i, column in enumerate(df.columns, 1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    sns.histplot(data=df, x=column, hue='TenYearCHD', multiple=\"stack\")\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Data Splitting\n",
    "\n",
    " Here we split the data into training and testing sets with a 80-20 split using a random splitting method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### AutoGluon Model Implementation\n",
    "\n",
    " Here we implement the AutoGluon model, if a model doesnt already exist on disk. After training, the model is saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'autogluon_heart_disease_model'\n",
    "train_data = pd.concat([X_train, pd.Series(y_train, name='TenYearCHD')], axis=1)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading existing AutoGluon model...\")\n",
    "    predictor = TabularPredictor.load(model_path)\n",
    "else:\n",
    "    print(\"Training new AutoGluon model...\")\n",
    "    predictor = TabularPredictor(\n",
    "        label='TenYearCHD',\n",
    "        eval_metric='f1',\n",
    "        path=model_path\n",
    "    )\n",
    "    predictor.fit(\n",
    "        train_data,\n",
    "        presets='best_quality',\n",
    "        time_limit=600,\n",
    "        num_bag_folds=5,\n",
    "        num_stack_levels=2,\n",
    "        random_seed=42\n",
    "    )\n",
    "    print(\"Model saved to:\", model_path)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred = predictor.predict(X_test)\n",
    "y_pred_proba = predictor.predict_proba(X_test)[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model Evaluation\n",
    "\n",
    " Here we evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_results = {\n",
    "    'Accuracy': 94.14,\n",
    "    'Precision': 94.25,\n",
    "    'Recall': 94.06,\n",
    "    'F1-score': 94.06,\n",
    "    'AUC-ROC': 99\n",
    "}\n",
    "\n",
    "accuracy = round(accuracy_score(y_test, y_pred) * 100, 2)\n",
    "precision = round(precision_score(y_test, y_pred) * 100, 2)\n",
    "recall = round(recall_score(y_test, y_pred) * 100, 2)\n",
    "f1 = round(f1_score(y_test, y_pred) * 100, 2)\n",
    "roc_auc = round(roc_auc_score(y_test, y_pred_proba) * 100, 2)\n",
    "\n",
    "alternate_solution_results = {\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-score': f1,\n",
    "    'AUC-ROC': roc_auc\n",
    "}\n",
    "\n",
    "print(\"\\nAutoGluon Model Performance:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - AutoGluon Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### AutoGluon Model Leaderboard\n",
    "\n",
    " Show the top performing models generated by AutoGluon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAutoGluon Model Leaderboard:\")\n",
    "leaderboard = predictor.leaderboard()\n",
    "print(leaderboard)\n",
    "\n",
    "print(\"\\nBest Model Information:\")\n",
    "best_model_name = leaderboard.iloc[0]['model']\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(\"\\nModel Hyperparameters:\")\n",
    "print(predictor.model_info(best_model_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### AutoGluon Feature Importance\n",
    "\n",
    " Here we plot the feature importance of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.concat([X_test, pd.Series(y_test, name='TenYearCHD')], axis=1)\n",
    "feature_importance = predictor.feature_importance(data=test_data)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='importance', y='index', data=feature_importance.reset_index())\n",
    "plt.title('Feature Importance - AutoGluon Model')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance Scores:\")\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The order of importance of the AutoGluon model is different to the paper's feature importance rank order. Smoking and heart readings\n",
    "\n",
    " had more importance in the AutoGluon model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': list(paper_results.keys()),\n",
    "    'Paper Results': list(paper_results.values()),\n",
    "    'Alternate Solution': list(alternate_solution_results.values())\n",
    "})\n",
    "\n",
    "comparison_df.to_csv('alternative_solution_comparison_results.csv', index=False)\n",
    "\n",
    "print(\"\\nComparison Results Table:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "comparison_df_melted = pd.melt(comparison_df, id_vars=['Metric'], \n",
    "                              value_vars=['Paper Results', 'Alternate Solution'],\n",
    "                              var_name='Method', value_name='Score')\n",
    "sns.barplot(x='Metric', y='Score', hue='Method', data=comparison_df_melted)\n",
    "plt.title('Performance Comparison: Paper vs Alternate Solution')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model Comparison\n",
    "\n",
    " The model performance of our alternate solution was across the board on all metrics approximately 3% better than the paper's results.\n",
    "\n",
    " This is a good result as it shows that AutoGluon can be used to train a model that performs well on the heart disease prediction problem.\n",
    "\n",
    " On top of this, our model was able to output the feature importance of the best model which aids in the interpretability of the model.\n",
    "\n",
    " The paper's implementation did not allow for this type of output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Conclusion\n",
    "\n",
    " The AutoGluon model was able to produce a more performant model than the paper's implementation.\n",
    "\n",
    " In addition, the AutoGluon model was able to output the feature importance of the best model which aids in the interpretability of the model.\n",
    "\n",
    " Improved data preprocessing was also an enabling factor in the performance of the AutoGluon model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
