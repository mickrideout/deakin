{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item 1 Dataset Splitting\n",
    "Load the dataset into a pandas dataset and perform train / test splitting using a random split and a group based split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the dataset from csv\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df.head()\n",
    "\n",
    "# Do the random train / test split with ratio of 70% / 30%\n",
    "rand_train_df, rand_test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "print(f\"Rand train set shape: {rand_train_df.shape}\")\n",
    "print(f\"Rand test set shape: {rand_test_df.shape}\")\n",
    "\n",
    "print('------------------------------')\n",
    "\n",
    "# Perform train test splitting based on X8 values\n",
    "\n",
    "# Print value counts of X8 variable to see distribution\n",
    "x8_counts = df['X8'].value_counts().sort_index()\n",
    "print(\"Value counts in X8 variable:\")\n",
    "print(x8_counts)\n",
    "print('------------------------------')\n",
    "\n",
    "\n",
    "test_x8_values = [1,2] # two values arbitrary chosen\n",
    "print(f\"X8 values in test set only {test_x8_values}\")\n",
    "\n",
    "group_test_df = df[df['X8'].isin(test_x8_values)]\n",
    "group_train_df = df[~df['X8'].isin(test_x8_values)]\n",
    "\n",
    "print(f\"Group train set shape: {group_train_df.shape}\")\n",
    "print(f\"Group test set shape: {group_test_df.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 1 Commentary\n",
    "\n",
    "Splitting a dataset into training / test splits using a random method is advisable as the dataset may contain an inherent order, and splitting based on default row position may lead to imbalanced train / test splits. Splitting the dataset by withholding two classes in the X8 variable is extremely problematic, as the training dataset would have no cases of the two withheld classes and therefore would not be able to learn an accurate model for these two classes. The upshot of this in this case is that a model trained on the group split datasets would perform poorly at predicting the heating or cooling loads when presented with the two withheld classes of X8.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item 2 Dataset Split Analysis\n",
    "\n",
    "Compare via bar charts, the effect of the differing split methods on the X6 (orientation) population in each of the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping for orientation values to their meanings\n",
    "orientation_mapping = {2: 'East', 3: 'South', 4: 'West', 5: 'North'}\n",
    "\n",
    "# Count the occurrences of each orientation in random split datasets\n",
    "rand_train_counts = rand_train_df['X6'].value_counts().sort_index()\n",
    "rand_test_counts = rand_test_df['X6'].value_counts().sort_index()\n",
    "\n",
    "# Count the occurrences of each orientation in group split datasets\n",
    "group_train_counts = group_train_df['X6'].value_counts().sort_index()\n",
    "group_test_counts = group_test_df['X6'].value_counts().sort_index()\n",
    "\n",
    "# Set up the figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for random split\n",
    "x = np.arange(len(orientation_mapping))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, rand_train_counts, width, label='Train')\n",
    "ax1.bar(x + width/2, rand_test_counts, width, label='Test')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([orientation_mapping[i] for i in sorted(orientation_mapping.keys())])\n",
    "ax1.set_xlabel('Building Orientation')\n",
    "ax1.set_ylabel('Number of Buildings')\n",
    "ax1.set_title('Building Orientations in Random Split')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot for group split\n",
    "ax2.bar(x - width/2, group_train_counts, width, label='Train')\n",
    "ax2.bar(x + width/2, group_test_counts, width, label='Test')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([orientation_mapping[i] for i in sorted(orientation_mapping.keys())])\n",
    "ax2.set_xlabel('Building Orientation')\n",
    "ax2.set_ylabel('Number of Buildings')\n",
    "ax2.set_title('Building Orientations in Group Split')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the actual counts for reference\n",
    "print(\"Random Split - Building Orientations:\")\n",
    "for orient_val, orient_name in orientation_mapping.items():\n",
    "    train_count = rand_train_counts.get(orient_val, 0)\n",
    "    test_count = rand_test_counts.get(orient_val, 0)\n",
    "    print(f\"{orient_name}: Train={train_count}, Test={test_count}\")\n",
    "\n",
    "print(\"\\nGroup Split - Building Orientations:\")\n",
    "for orient_val, orient_name in orientation_mapping.items():\n",
    "    train_count = group_train_counts.get(orient_val, 0)\n",
    "    test_count = group_test_counts.get(orient_val, 0)\n",
    "    print(f\"{orient_name}: Train={train_count}, Test={test_count}\")\n",
    "\n",
    "# Manually check the counts of X6 for one X8 group (1 in this case) as I would have thought removing groups where the glazing is 1 (north) would have affected X6 counts but this wasnt the case.\n",
    "group_test_x8_1_counts = group_test_df[group_test_df['X8'] == 1]['X6'].value_counts().sort_index()\n",
    "\n",
    "print(\"\\nGroup Test - Building Orientations where X8 = 1:\")\n",
    "for orient_val, orient_name in orientation_mapping.items():\n",
    "    count = group_test_x8_1_counts.get(orient_val, 0)\n",
    "    print(f\"{orient_name}: Count={count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 2 Commentary \n",
    "\n",
    "Both random splits and group splits produced dataset segregations that did not result in under or over representation with respect to the X6 (Orientation) variable. This result is counter intuitive for the group split datasets, as the holding out of two orientations (north and east) for glazing would imply that the building orientation X6 variable should have been affected by this splitting method. Manual investigation of one X8 group (group 1, north) shows that the value count of the X6 (Orientation) variable had a uniform distribution of building orientations. The same was true for all other non zero X8 groups. Therefore the group split datasets have uniform counts for X6 when grouped by a X8 class.\n",
    "Many machine learning algorithms expect that variables have a normal distribution, so any deviation from this will impact predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item 3 Linear Regression \n",
    "\n",
    "In this item we use the random train / test split dataset and fit a linear regression model. We then evaluate that model using the test dataset to determine mean squared error, root mean squared error and R^2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8']\n",
    "target_feature = 'Y1'\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Define inputs out and out feature arrays\n",
    "X_train = rand_train_df[input_features]\n",
    "y_train = rand_train_df[target_feature]\n",
    "\n",
    "X_test = rand_test_df[input_features]\n",
    "y_test = rand_test_df[target_feature]\n",
    "\n",
    "# Fit the model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set to evaluate the model\n",
    "y_test_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Linear Regression Results:\")\n",
    "print(f\"Mean Squared Error: {train_mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {train_rmse:.4f}\")\n",
    "print(f\"R^2 Score: {train_r2:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 3 Commentary\n",
    "\n",
    "Mean Squared Error is the average squared difference between the observed and predicted values. The unit is the square of the unit of the target variable. A lower number is better, but further analysis and comparison to the distribution of the target variable is needed. Root Mean Squared Error is simply the square root of MSE and the same comment applies. R^2 score measures the proportion of variance in the input features that is predictable in the target feature. A score of 1 is the highest achievable. The R^2 score of 0.9128 shows that 91% of variance of the inputs features is predicted by the output target variable therefore the model have has a high predict power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 4 Principal Component Analysis\n",
    "\n",
    "Use PCA to reduce the dimensionality of the input feature set and evaluate the PCA dataset against the original dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Fit the pca model\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Print explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(\"Explained variance ratio by the 3 principal components:\")\n",
    "print(explained_variance)\n",
    "print(f\"Total variance explained: {sum(explained_variance):.4f}\")\n",
    "\n",
    "# Linear regression model on PCA dataset\n",
    "pca_model = LinearRegression()\n",
    "pca_model.fit(X_train_pca, y_train)\n",
    "\n",
    "y_test_pca_pred = pca_model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model performance\n",
    "pca_mse = mean_squared_error(y_test, y_test_pca_pred)\n",
    "pca_rmse = np.sqrt(pca_mse)\n",
    "pca_r2 = r2_score(y_test, y_test_pca_pred)\n",
    "\n",
    "print(\"\\nLinear Regression Results after PCA:\")\n",
    "print(f\"Mean Squared Error: {pca_mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {pca_rmse:.4f}\")\n",
    "print(f\"R^2 Score: {pca_r2:.4f}\")\n",
    "\n",
    "print(\"\\nComparison of model performance:\")\n",
    "print(f\"Original MSE: {train_mse:.4f} vs PCA MSE: {pca_mse:.4f}\")\n",
    "print(f\"Original R^2: {train_r2:.4f} vs PCA R^2: {pca_r2:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 4 Commentary\n",
    "\n",
    "The R^2 score for the original linear model was 0.9128 and for the PCA dataset the linear model's R^2 score was 0.7924. Therefore the PCA reduced dataset hurt performance. Some of the pros of reducing dimensionality is that it can simplify model, ignore potentially harmful features and make analysis simpler. Some cons are that it cannot capture 100% of the original variance of the underlying data. There is also a level of abstraction introduced as components are a combination of original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item 5 Ridge Regression\n",
    "\n",
    "The task is to use Ridge Regression on the PCA dataset and to compare the results with linear regression results on the PCA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ridge_model = Ridge(alpha=10)\n",
    "ridge_model.fit(X_train_pca, y_train)\n",
    "\n",
    "y_test_ridge_pred = ridge_model.predict(X_test_pca)\n",
    "\n",
    "# Calc eval\n",
    "ridge_mse = mean_squared_error(y_test, y_test_ridge_pred)\n",
    "ridge_rmse = np.sqrt(ridge_mse)\n",
    "ridge_r2 = r2_score(y_test, y_test_ridge_pred)\n",
    "\n",
    "print(\"\\nRidge Regression Results on PCA dataset:\")\n",
    "print(f\"Mean Squared Error: {ridge_mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {ridge_rmse:.4f}\")\n",
    "print(f\"R^2 Score: {ridge_r2:.4f}\")\n",
    "\n",
    "print(\"\\nComparison of Ridge vs Linear Regression on PCA dataset:\")\n",
    "print(f\"Linear Regression PCA MSE: {pca_mse:.4f} vs Ridge PCA MSE: {ridge_mse:.4f}\")\n",
    "print(f\"Linear Regression PCA R^2: {pca_r2:.4f} vs Ridge PCA R^2: {ridge_r2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 5 Commentary\n",
    "\n",
    "The addition of regularisation did not improve performance as the MSE and R^2 were almost identical. Regularisation is often used to address multicolinearity which had already been addressed by principal component analysis as the components created are not correlated. In general regularisation may still be useful after PCA if a large number of components are retained as overfitting may still occur. Components with small eignvalues (variance) may also cause numerical instability so regularisation would help to address this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
