{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6.2 Investigation of Microclimate Sensors Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and display the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv('microclimate-sensors-data.csv')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the features do not agree on the non-null row count so preprocess is required to impute missing values and restrict the dataset to the number of non-null rows of the SensorLocation target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of non-null values in SensorLocation: {df['SensorLocation'].count()}\")\n",
    "\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Restrict to non-null SensorLocation\n",
    "df_clean = df.dropna(subset=['SensorLocation'])\n",
    "print(f\"\\nShape after restricting to non-null SensorLocation: {df_clean.shape}\")\n",
    "\n",
    "# Approach for missing values differs for numeric and categorical columns\n",
    "numeric_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = df_clean.select_dtypes(include=['object']).columns.drop('SensorLocation') if 'SensorLocation' in df_clean.columns else df_clean.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute missing values\n",
    "for col in numeric_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        mean_val = df_clean[col].mean()\n",
    "        df_clean[col].fillna(mean_val, inplace=True)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        mode_val = df_clean[col].mode()[0]\n",
    "        df_clean[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "# Check the results\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df_clean.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 1 Optimal Number of Groups\n",
    "\n",
    "Here we aim to answer what is the optimal number of groups and what effect dimensionality reduction has on clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 1a - Unique Number of Target Classes\n",
    "\n",
    "Since we have the ground truth values in a categorical value already, the ideal number of groups would be the unique number of 'sensor location' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sensor location counts\n",
    "unique_locations = df_clean['SensorLocation'].nunique()\n",
    "print(f\"Number of unique sensor locations: {unique_locations}\")\n",
    "\n",
    "location_counts = df_clean['SensorLocation'].value_counts()\n",
    "print(\"\\nUnique sensor locations and their counts:\")\n",
    "print(location_counts)\n",
    "\n",
    "# plot a bar chart of the sensor location counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "location_counts.plot(kind='bar')\n",
    "plt.title('Distribution of Sensor Locations')\n",
    "plt.xlabel('Sensor Location')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Scaling**\n",
    "As cluster algorithm utilise distance metrics, we need to ensure that all numeric variables are standardised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scale the numeric columns\n",
    "scaler = StandardScaler()\n",
    "df_clean[numeric_cols] = scaler.fit_transform(df_clean[numeric_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 1a - Optimal Cluster Count via Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans from sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Calculate the within-cluster sum of squares (WCSS) for different numbers of clusters\n",
    "wcss = []\n",
    "max_clusters = 15  # Try up to 15 clusters\n",
    "\n",
    "for i in range(1, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=i, init='random', max_iter=300, n_init=10, random_state=42)\n",
    "    kmeans.fit(df_clean[numeric_cols])\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Find the optimal number of clusters using the elbow method\n",
    "# We'll use the KneeLocator from kneed package if available\n",
    "try:\n",
    "    from kneed import KneeLocator\n",
    "    kl = KneeLocator(range(1, max_clusters + 1), wcss, curve='convex', direction='decreasing')\n",
    "    optimal_k = kl.elbow\n",
    "except ImportError:\n",
    "    # If kneed is not available, we'll use a simple heuristic\n",
    "    # Calculate the rate of change in WCSS\n",
    "    diffs = [wcss[i-1] - wcss[i] for i in range(1, len(wcss))]\n",
    "    # Find where the rate of change starts to slow down significantly\n",
    "    optimal_k = diffs.index(min([d for d in diffs if d > sum(diffs)/len(diffs)/2])) + 2\n",
    "\n",
    "# Plot the Elbow Method graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, max_clusters + 1), wcss, marker='o', linestyle='-')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, max_clusters + 1))\n",
    "\n",
    "# Add vertical lines for both ground truth and optimal elbow point\n",
    "plt.axvline(x=unique_locations, color='r', linestyle='--', \n",
    "            label=f'Ground Truth: {unique_locations} (SensorLocation)')\n",
    "plt.axvline(x=optimal_k, color='g', linestyle='--', \n",
    "            label=f'Optimal Elbow: {optimal_k}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal number of clusters determined by elbow method: {optimal_k}\")\n",
    "print(f\"Ground truth number of clusters (unique SensorLocation values): {unique_locations}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal kmeans cluster count obtained by the elbow method was 5. Compare it to 7 (the number of unique classes in sensor location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary metrics for cluster evaluation\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Get the ground truth labels from SensorLocation\n",
    "ground_truth_labels = df_clean['SensorLocation'].values\n",
    "\n",
    "# Perform K-means clustering with 5 clusters (optimal from elbow method)\n",
    "kmeans_5 = KMeans(n_clusters=5, init='random', max_iter=300, n_init=10, random_state=42)\n",
    "cluster_labels_5 = kmeans_5.fit_predict(df_clean[numeric_cols])\n",
    "\n",
    "# Perform K-means clustering with 7 clusters (ground truth)\n",
    "kmeans_7 = KMeans(n_clusters=7, init='random', max_iter=300, n_init=10, random_state=42)\n",
    "cluster_labels_7 = kmeans_7.fit_predict(df_clean[numeric_cols])\n",
    "\n",
    "# Compute evaluation metrics for 5 clusters\n",
    "ari_5 = adjusted_rand_score(ground_truth_labels, cluster_labels_5)\n",
    "nmi_5 = normalized_mutual_info_score(ground_truth_labels, cluster_labels_5)\n",
    "\n",
    "# Compute evaluation metrics for 7 clusters\n",
    "ari_7 = adjusted_rand_score(ground_truth_labels, cluster_labels_7)\n",
    "nmi_7 = normalized_mutual_info_score(ground_truth_labels, cluster_labels_7)\n",
    "\n",
    "# Print results\n",
    "print(\"K-means with 5 clusters (optimal from elbow method):\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_5:.4f}\")\n",
    "print(f\"Normalised Mutual Information (NMI): {nmi_5:.4f}\")\n",
    "print(\"\\nK-means with 7 clusters (ground truth):\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari_7:.4f}\")\n",
    "print(f\"Normalised Mutual Information (NMI): {nmi_7:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing DBSCAN for cluster discovery\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Function to find optimal epsilon using k-distance graph\n",
    "def find_optimal_eps(data, k=5):\n",
    "    # Calculate distances to k nearest neighbors for each point\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(data)\n",
    "    distances, _ = neigh.kneighbors(data)\n",
    "    \n",
    "    # Sort distances to kth neighbor in ascending order\n",
    "    k_distances = np.sort(distances[:, k-1])\n",
    "    \n",
    "    # Plot k-distance graph\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(len(k_distances)), k_distances)\n",
    "    plt.xlabel('Data Points (sorted by distance)')\n",
    "    plt.ylabel(f'Distance to {k}th Nearest Neighbor')\n",
    "    plt.title('K-Distance Graph for DBSCAN Epsilon Parameter Selection')\n",
    "    \n",
    "    # Add a grid to help identify the \"elbow\"\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return k_distances\n",
    "\n",
    "# Find optimal epsilon value\n",
    "k_distances = find_optimal_eps(df_clean[numeric_cols])\n",
    "\n",
    "# Based on the k-distance graph, we can identify the \"elbow\" point\n",
    "# Let's try a range of epsilon values and min_samples\n",
    "# eps_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "# min_samples_values = [5, 10, 15]\n",
    "eps_values = [0.3]\n",
    "min_samples_values = [10]\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        # Apply DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        cluster_labels = dbscan.fit_predict(df_clean[numeric_cols])\n",
    "        \n",
    "        # Count number of clusters (excluding noise points labeled as -1)\n",
    "        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "        noise_points = list(cluster_labels).count(-1)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        if n_clusters > 0:  # Only calculate metrics if clusters were found\n",
    "            ari = adjusted_rand_score(ground_truth_labels, cluster_labels)\n",
    "            nmi = normalized_mutual_info_score(ground_truth_labels, cluster_labels)\n",
    "        else:\n",
    "            ari = 0\n",
    "            nmi = 0\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'eps': eps,\n",
    "            'min_samples': min_samples,\n",
    "            'n_clusters': n_clusters,\n",
    "            'noise_points': noise_points,\n",
    "            'noise_percentage': noise_points / len(cluster_labels) * 100,\n",
    "            'ari': ari,\n",
    "            'nmi': nmi\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display results sorted by ARI (higher is better)\n",
    "print(\"DBSCAN Results sorted by ARI:\")\n",
    "print(results_df.sort_values('ari', ascending=False).head(10))\n",
    "\n",
    "# Select the best parameter combination based on ARI\n",
    "best_params = results_df.loc[results_df['ari'].idxmax()]\n",
    "print(\"\\nBest DBSCAN Parameters:\")\n",
    "print(f\"Epsilon: {best_params['eps']}\")\n",
    "print(f\"Min Samples: {best_params['min_samples']}\")\n",
    "print(f\"Number of Clusters: {best_params['n_clusters']}\")\n",
    "print(f\"Adjusted Rand Index: {best_params['ari']:.4f}\")\n",
    "print(f\"Normalised Mutual Information: {best_params['nmi']:.4f}\")\n",
    "print(f\"Noise Points: {best_params['noise_points']} ({best_params['noise_percentage']:.2f}%)\")\n",
    "\n",
    "# Apply DBSCAN with the best parameters\n",
    "best_dbscan = DBSCAN(eps=best_params['eps'], min_samples=int(best_params['min_samples']))\n",
    "best_cluster_labels = best_dbscan.fit_predict(df_clean[numeric_cols])\n",
    "\n",
    "# Visualise cluster distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "cluster_counts = Counter(best_cluster_labels)\n",
    "labels = [f\"Cluster {i}\" if i >= 0 else \"Noise\" for i in sorted(cluster_counts.keys())]\n",
    "counts = [cluster_counts[i] for i in sorted(cluster_counts.keys())]\n",
    "\n",
    "plt.bar(labels, counts)\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Data Points')\n",
    "plt.title('DBSCAN Cluster Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare with ground truth\n",
    "print(\"\\nComparison with Ground Truth:\")\n",
    "print(f\"DBSCAN found {best_params['n_clusters']} clusters (excluding noise)\")\n",
    "print(f\"Ground truth has {unique_locations} unique sensor locations\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
