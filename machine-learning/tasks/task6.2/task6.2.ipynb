{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from kneed import KneeLocator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6.2 Investigation of Microclimate Sensors Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and display the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv('microclimate-sensors-data.csv')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the features do not agree on the non-null row count so preprocess is required to impute missing values and restrict the dataset to the number of non-null rows of the SensorLocation target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of non-null values in SensorLocation: {df['SensorLocation'].count()}\")\n",
    "\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Restrict to non-null SensorLocation\n",
    "df_clean = df.dropna(subset=['SensorLocation', 'LatLong'])\n",
    "print(f\"\\nShape after restricting to non-null SensorLocation: {df_clean.shape}\")\n",
    "\n",
    "# Split LatLong into separate Latitude and Longitude columns\n",
    "df_clean[['Latitude', 'Longitude']] = df_clean['LatLong'].str.split(',', expand=True).astype(float)\n",
    "df_clean.drop(columns=['LatLong'], inplace=True)\n",
    "\n",
    "# Drop all columns except SensorLocation, Latitude, and Longitude\n",
    "#df_clean = df_clean[['SensorLocation', 'Latitude', 'Longitude']]\n",
    "\n",
    "\n",
    "\n",
    "# Approach for missing values differs for numeric and categorical columns\n",
    "numeric_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = df_clean.select_dtypes(include=['object']).columns.drop('SensorLocation') if 'SensorLocation' in df_clean.columns else df_clean.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute missing values\n",
    "for col in numeric_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        mean_val = df_clean[col].mean()\n",
    "        df_clean[col].fillna(mean_val, inplace=True)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        mode_val = df_clean[col].mode()[0]\n",
    "        df_clean[col].fillna(mode_val, inplace=True)\n",
    "\n",
    "# Check the results\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df_clean.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics Function\n",
    "\n",
    "Here we define the evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_clustering(true_labels, pred_labels):\n",
    "    \"\"\"\n",
    "    Print clustering evaluation metrics: ARI, NMI, and Silhouette Score (if possible).\n",
    "    Optionally prints number of clusters and noise points for DBSCAN-like algorithms.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    print(\"Clustering Evaluation Results:\")\n",
    "    \n",
    "    # ARI\n",
    "    ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "    print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "    \n",
    "    # NMI\n",
    "    nmi = normalized_mutual_info_score(true_labels, pred_labels)\n",
    "    print(f\"Normalized Mutual Information (NMI): {nmi:.4f}\")\n",
    "    \n",
    "    # Silhouette Score (only if features and at least 2 clusters)\n",
    "    n_clusters = len(set(pred_labels)) - (1 if -1 in pred_labels else 0)\n",
    "\n",
    "    \n",
    "    # Calculate purity score\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(true_labels, pred_labels)\n",
    "    purity = np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "    print(f\"Purity Score: {purity:.4f}\")\n",
    "    \n",
    "    # For DBSCAN-like: print number of clusters and noise points\n",
    "    if -1 in pred_labels:\n",
    "        noise_points = list(pred_labels).count(-1)\n",
    "        print(f\"Number of clusters (excluding noise): {n_clusters}\")\n",
    "        print(f\"Noise points: {noise_points} ({noise_points/len(pred_labels)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"Number of clusters: {n_clusters}\")\n",
    "    print()\n",
    "\n",
    "    # Create dictionary to store metrics\n",
    "    metrics_dict = {\n",
    "        'ari': ari,\n",
    "        'nmi': nmi,\n",
    "        'purity': purity,\n",
    "        'n_clusters': n_clusters\n",
    "    }\n",
    "    \n",
    "    # Add noise points metrics if applicable\n",
    "    if -1 in pred_labels:\n",
    "        noise_points = list(pred_labels).count(-1)\n",
    "        metrics_dict['noise_points'] = noise_points\n",
    "        metrics_dict['noise_percentage'] = noise_points/len(pred_labels)*100\n",
    "        \n",
    "    # # Add silhouette score if possible\n",
    "    # if features is not None and n_clusters >= 2:\n",
    "    #     try:\n",
    "    #         silhouette_avg = silhouette_score(features, pred_labels)\n",
    "    #         print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "    #         metrics_dict['silhouette'] = silhouette_avg\n",
    "    #     except:\n",
    "    #         pass\n",
    "            \n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Kmeans Clustering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmeans_clustering(input_dataframe, n_clusters):\n",
    "    \"\"\"\n",
    "    Performs KMeans clustering on the input dataframe and evaluates the results.\n",
    "    \n",
    "    Args:\n",
    "        input_dataframe: DataFrame containing the features to cluster\n",
    "        n_clusters: Number of clusters to create\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get numeric columns only\n",
    "    numeric_cols = input_dataframe.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    # Initialize and fit KMeans on numeric columns only\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(input_dataframe[numeric_cols])\n",
    "    \n",
    "    # Get true labels from SensorLocation\n",
    "    true_labels = input_dataframe.index.get_level_values('SensorLocation') if isinstance(input_dataframe.index, pd.MultiIndex) else input_dataframe['SensorLocation']\n",
    "    \n",
    "    # Evaluate clustering\n",
    "    print(f\"\\nKMeans Clustering Results (k={n_clusters}):\")\n",
    "    print(\"-\" * 40)\n",
    "    evaluate_clustering(true_labels, cluster_labels)\n",
    "    \n",
    "    return cluster_labels, kmeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 1 Optimal Number of Groups\n",
    "\n",
    "Here we aim to answer what is the optimal number of groups and what effect dimensionality reduction has on clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 1a - Unique Number of Target Classes\n",
    "\n",
    "Since we have the ground truth values in a categorical value already, the ideal number of groups would be the unique number of 'sensor location' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sensor location counts\n",
    "unique_locations = df_clean['SensorLocation'].nunique()\n",
    "print(f\"Number of unique sensor locations: {unique_locations}\")\n",
    "\n",
    "location_counts = df_clean['SensorLocation'].value_counts()\n",
    "print(\"\\nUnique sensor locations and their counts:\")\n",
    "print(location_counts)\n",
    "\n",
    "# plot a bar chart of the sensor location counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "location_counts.plot(kind='bar')\n",
    "plt.title('Distribution of Sensor Locations')\n",
    "plt.xlabel('Sensor Location')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Scaling**\n",
    "As cluster algorithm utilise distance metrics, we need to ensure that all numeric variables are standardised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scale the numeric columns\n",
    "scaler = StandardScaler()\n",
    "df_clean[numeric_cols] = scaler.fit_transform(df_clean[numeric_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 1a - Optimal Cluster Count via Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans from sklearn\n",
    "\n",
    "\n",
    "# Calculate the within-cluster sum of squares (WCSS) for different numbers of clusters\n",
    "wcss = []\n",
    "max_clusters = 15  # Try up to 15 clusters\n",
    "\n",
    "for i in range(1, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=i, init='random', max_iter=300, n_init=10, random_state=42)\n",
    "    kmeans.fit(df_clean[numeric_cols])\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Find the optimal number of clusters using the elbow method\n",
    "# We'll use the KneeLocator from kneed package if available\n",
    "try:\n",
    "\n",
    "    kl = KneeLocator(range(1, max_clusters + 1), wcss, curve='convex', direction='decreasing')\n",
    "    optimal_k = kl.elbow\n",
    "except ImportError:\n",
    "    # If kneed is not available, we'll use a simple heuristic\n",
    "    # Calculate the rate of change in WCSS\n",
    "    diffs = [wcss[i-1] - wcss[i] for i in range(1, len(wcss))]\n",
    "    # Find where the rate of change starts to slow down significantly\n",
    "    optimal_k = diffs.index(min([d for d in diffs if d > sum(diffs)/len(diffs)/2])) + 2\n",
    "\n",
    "# Plot the Elbow Method graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, max_clusters + 1), wcss, marker='o', linestyle='-')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, max_clusters + 1))\n",
    "\n",
    "# Add vertical lines for both ground truth and optimal elbow point\n",
    "plt.axvline(x=unique_locations, color='r', linestyle='--', \n",
    "            label=f'Ground Truth: {unique_locations} (SensorLocation)')\n",
    "plt.axvline(x=optimal_k, color='g', linestyle='--', \n",
    "            label=f'Optimal Elbow: {optimal_k}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal number of clusters determined by elbow method: {optimal_k}\")\n",
    "print(f\"Ground truth number of clusters (unique SensorLocation values): {unique_locations}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal kmeans cluster count obtained by the elbow method was 5. Compare it to 7 (the number of unique classes in sensor location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering results\n",
    "print(f\"K-means with {optimal_k} clusters (optimal from elbow method):\")\n",
    "perform_kmeans_clustering(df_clean, optimal_k)\n",
    "\n",
    "print(\"\\nK-means with 11 clusters (ground truth):\")\n",
    "perform_kmeans_clustering(df_clean, 11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 1b\n",
    "\n",
    "Feature Reduction's Effect on CLustering Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Feature Selection for K-means Clustering\n",
    "\n",
    "# We'll use SelectKBest with mutual_info_classif to select the most informative features\n",
    "\n",
    "\n",
    "def evaluate_feature_subset(X, n_features, n_clusters=11):\n",
    "    # Select top n features\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=n_features)\n",
    "    X_selected = selector.fit_transform(X, kmeans.labels_)\n",
    "    \n",
    "    # Perform k-means clustering\n",
    "    kmeans_subset = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans_subset.fit_predict(X_selected)\n",
    "    \n",
    "    # Calculate ARI score\n",
    "    score = adjusted_rand_score(df_clean['SensorLocation'], labels)\n",
    "    \n",
    "    # Get selected feature names\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    \n",
    "    return score, selected_features\n",
    "\n",
    "# Try different numbers of features\n",
    "feature_range = range(2, len(numeric_cols) + 1)\n",
    "scores = []\n",
    "best_score = -1\n",
    "best_features = None\n",
    "\n",
    "for n_features in feature_range:\n",
    "    score, features = evaluate_feature_subset(df_clean[numeric_cols], n_features)\n",
    "    scores.append(score)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_features = features\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(feature_range, scores, marker='o')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Adjusted Rand Index')\n",
    "plt.title('Feature Selection Impact on Clustering Performance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best ARI score: {best_score:.3f}\")\n",
    "print(f\"Best performing features ({len(best_features)}):\")\n",
    "for feature in best_features:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "# Perform final clustering with best features\n",
    "X_best = df_clean[best_features]\n",
    "final_kmeans = KMeans(n_clusters=11, random_state=42)\n",
    "final_labels = final_kmeans.fit_predict(X_best)\n",
    "print(\"\\nFinal clustering performance metrics:\")\n",
    "print(f\"ARI Score: {adjusted_rand_score(df_clean['SensorLocation'], final_labels):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best = df_clean[best_features + ['SensorLocation']]\n",
    "numeric_cols_best = ['Latitude', 'Longitude']\n",
    "df_best.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform kmeans on the dataframe with only latitude and longitude as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_kmeans_clustering(df_best, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on numeric columns while preserving SensorLocation\n",
    "\n",
    "\n",
    "# Create PCA object with 4 components\n",
    "pca = PCA(n_components=4)\n",
    "\n",
    "# Fit and transform numeric data\n",
    "pca_result = pca.fit_transform(df_clean[numeric_cols])\n",
    "\n",
    "# Create new dataframe with PCA results\n",
    "pca_df = pd.DataFrame(\n",
    "    pca_result,\n",
    "    columns=['PC1', 'PC2', 'PC3', 'PC4']\n",
    ")\n",
    "\n",
    "# Add back SensorLocation\n",
    "pca_df['SensorLocation'] = df_clean['SensorLocation'].values\n",
    "\n",
    "# Print explained variance ratio\n",
    "print(\"Explained variance ratio:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"\\nCumulative explained variance ratio:\")\n",
    "print(np.cumsum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_kmeans_clustering(pca_df, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 2\n",
    "\n",
    "Alternate solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Implementing DBSCAN for cluster discovery\n",
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from collections import Counter\n",
    "\n",
    "# # Function to find optimal epsilon using k-distance graph\n",
    "# def find_optimal_eps(data, k=5):\n",
    "#     # Calculate distances to k nearest neighbors for each point\n",
    "#     neigh = NearestNeighbors(n_neighbors=k)\n",
    "#     neigh.fit(data)\n",
    "#     distances, _ = neigh.kneighbors(data)\n",
    "    \n",
    "#     # Sort distances to kth neighbor in ascending order\n",
    "#     k_distances = np.sort(distances[:, k-1])\n",
    "    \n",
    "#     # Plot k-distance graph\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.plot(range(len(k_distances)), k_distances)\n",
    "#     plt.xlabel('Data Points (sorted by distance)')\n",
    "#     plt.ylabel(f'Distance to {k}th Nearest Neighbor')\n",
    "#     plt.title('K-Distance Graph for DBSCAN Epsilon Parameter Selection')\n",
    "    \n",
    "#     # Add a grid to help identify the \"elbow\"\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "    \n",
    "#     return k_distances\n",
    "\n",
    "# # Find optimal epsilon value\n",
    "# k_distances = find_optimal_eps(df_best[numeric_cols_best])\n",
    "\n",
    "\n",
    "# # Common eps values to try, ranging from small to large neighborhood sizes\n",
    "# #eps_values = [0.1, 0.2, 0.4, 0.5, 0.75, 1.0]\n",
    "# eps_values = [0.75]\n",
    "\n",
    "# # Common min_samples values to try, representing different density requirements\n",
    "# #min_samples_values = [10, 15, 20]\n",
    "# min_samples_values = [20]\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for eps in eps_values:\n",
    "#     for min_samples in min_samples_values:\n",
    "#         # Apply DBSCAN\n",
    "#         dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "#         cluster_labels = dbscan.fit_predict(df_best[numeric_cols_best])\n",
    "        \n",
    "#         # Count number of clusters (excluding noise points labeled as -1)\n",
    "#         n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "#         noise_points = list(cluster_labels).count(-1)\n",
    "        \n",
    "#         # Calculate evaluation metrics using evaluate_clustering function\n",
    "#         if n_clusters > 0:  # Only calculate metrics if clusters were found\n",
    "#             metrics = evaluate_clustering(ground_truth_labels, cluster_labels)\n",
    "#         else:\n",
    "#             print(\"No clusters found\")\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierachical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply BIRCH Clustering (memory efficient hierarchical clustering)\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "# Try different numbers of clusters \n",
    "n_clusters_values = [11]  # Including 11 since KMeans worked well with it\n",
    "results_hierarchical = []\n",
    "\n",
    "for n_clusters in n_clusters_values:\n",
    "    # Apply BIRCH clustering\n",
    "    # threshold and branching_factor control memory usage\n",
    "    birch = Birch(n_clusters=n_clusters, \n",
    "                  threshold=0.5,  # Controls subcluster diameter threshold\n",
    "                  branching_factor=50)  # Controls number of subclusters per node\n",
    "    cluster_labels = birch.fit_predict(df_best[numeric_cols_best])\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    evaluate_clustering(df_best['SensorLocation'], cluster_labels)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 3 Shape Based Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from tslearn.clustering import KShape\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "# Prepare time series data\n",
    "# Select numeric columns for time series analysis\n",
    "ts_cols = ['AirTemperature', 'RelativeHumidity', 'AtmosphericPressure', 'PM25', 'PM10', 'Noise']\n",
    "ts_data = df_clean[ts_cols].values\n",
    "\n",
    "# Reshape data for k-shape clustering (samples, timestamps, features)\n",
    "ts_data = ts_data.reshape(len(ts_data), 1, -1)\n",
    "\n",
    "# Scale the time series data\n",
    "scaler = TimeSeriesScalerMeanVariance()\n",
    "ts_data_scaled = scaler.fit_transform(ts_data)\n",
    "\n",
    "# Initialize lists to store results\n",
    "results_kshape = []\n",
    "\n",
    "ks = KShape(n_clusters=11, random_state=42)\n",
    "cluster_labels = ks.fit_predict(ts_data_scaled)\n",
    "\n",
    "evaluate_clustering(df_best['SensorLocation'], cluster_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapelet-based Time Series Clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from tslearn.shapelets import ShapeletModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare the data\n",
    "X = df_clean[ts_cols].values\n",
    "y = df_clean['SensorLocation'].values  \n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Initialize and train the shapelet model\n",
    "n_shapelets_per_size = {\n",
    "    \"n_shapelets_per_size\": 5,  # Number of shapelets to learn per shapelet length\n",
    "    \"min_shapelet_length\": 3,    # Minimum length of shapelets\n",
    "    \"max_shapelet_length\": 10    # Maximum length of shapelets\n",
    "}\n",
    "\n",
    "shapelet_model = ShapeletModel(\n",
    "    n_shapelets_per_size=n_shapelets_per_size,\n",
    "    optimizer='sgd',\n",
    "    weight_regularizer=.01,\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "shapelet_model.fit(X, y_encoded)\n",
    "\n",
    "# Get the learned shapelets\n",
    "shapelets = shapelet_model.shapelets_\n",
    "\n",
    "# Transform the data using learned shapelets\n",
    "X_transformed = shapelet_model.transform(X)\n",
    "\n",
    "# Apply KMeans clustering on the transformed data\n",
    "n_clusters = best_n_clusters  # Using the same number of clusters as before\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "shapelet_clusters = kmeans.fit_predict(X_transformed)\n",
    "\n",
    "evaluate_clustering(df_best['SensorLocation'], shapelet_clusters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
