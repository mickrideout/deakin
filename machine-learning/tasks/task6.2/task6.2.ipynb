{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n",
    "from sklearn.cluster import KMeans, OPTICS\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import Birch\n",
    "from tslearn.clustering import KShape\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "from kneed import KneeLocator\n",
    "from sklearn.mixture import GaussianMixture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6.2 Investigation of Microclimate Sensors Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and display the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv('microclimate-sensors-data.csv')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the features do not agree on the non-null row count so preprocess is required to impute missing values and restrict the dataset to the number of non-null rows of the SensorLocation target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of non-null values in SensorLocation: {df['SensorLocation'].count()}\")\n",
    "\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Restrict to non-null SensorLocation\n",
    "df_clean = df.dropna(subset=['SensorLocation', 'LatLong'])\n",
    "print(f\"\\nShape after restricting to non-null SensorLocation: {df_clean.shape}\")\n",
    "\n",
    "# Split LatLong into separate Latitude and Longitude columns\n",
    "df_clean[['Latitude', 'Longitude']] = df_clean['LatLong'].str.split(',', expand=True).astype(float)\n",
    "df_clean.drop(columns=['LatLong'], inplace=True)\n",
    "\n",
    "\n",
    "numeric_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = df_clean.select_dtypes(include=['object']).columns.drop('SensorLocation') if 'SensorLocation' in df_clean.columns else df_clean.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Impute missing values\n",
    "for col in numeric_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        mean_val = df_clean[col].mean()\n",
    "        df_clean[col].fillna(mean_val, inplace=True)\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df_clean.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All missing values have been taken care of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics Function\n",
    "\n",
    "Here we define the evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_clustering(true_labels, pred_labels, algorithm_name):\n",
    "\n",
    "    \n",
    "    print(f\"Clustering Evaluation Results for {algorithm_name}:\")\n",
    "    \n",
    "    # ARI\n",
    "    ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "    print(f\"Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "    \n",
    "    # NMI\n",
    "    nmi = normalized_mutual_info_score(true_labels, pred_labels)\n",
    "    print(f\"Normalised Mutual Information (NMI): {nmi:.4f}\")\n",
    "    \n",
    "    n_clusters = len(set(pred_labels)) - (1 if -1 in pred_labels else 0)\n",
    "\n",
    "    \n",
    "    # Calculate purity score\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(true_labels, pred_labels)\n",
    "    purity = np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "    print(f\"Purity Score: {purity:.4f}\")\n",
    "    \n",
    "    # For DBSCAN-like: print number of clusters and noise points\n",
    "    if -1 in pred_labels:\n",
    "        noise_points = list(pred_labels).count(-1)\n",
    "        print(f\"Number of clusters (excluding noise): {n_clusters}\")\n",
    "        print(f\"Noise points: {noise_points} ({noise_points/len(pred_labels)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"Number of clusters: {n_clusters}\")\n",
    "    print()\n",
    "\n",
    "    # dictionary of results\n",
    "    metrics_dict = {\n",
    "        'algorithm': algorithm_name,\n",
    "        'ari': ari,\n",
    "        'nmi': nmi,\n",
    "        'purity': purity,\n",
    "        'n_clusters': n_clusters\n",
    "    }\n",
    "    \n",
    "    if -1 in pred_labels:\n",
    "        noise_points = list(pred_labels).count(-1)\n",
    "        metrics_dict['noise_points'] = noise_points\n",
    "        metrics_dict['noise_percentage'] = noise_points/len(pred_labels)*100\n",
    "            \n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Kmeans Clustering Function\n",
    "\n",
    "We are going to run kmeans a few times so create a function to do this, evaludate and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmeans_clustering(input_dataframe, n_clusters, algorithm_name, visualise=False, plot_title=None):\n",
    "\n",
    "    \n",
    "    # Get numeric columns only\n",
    "    numeric_cols = input_dataframe.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    # Initialize and fit KMeans on numeric columns only\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(input_dataframe[numeric_cols])\n",
    "    \n",
    "    # Get true labels from SensorLocation\n",
    "    true_labels = input_dataframe.index.get_level_values('SensorLocation') if isinstance(input_dataframe.index, pd.MultiIndex) else input_dataframe['SensorLocation']\n",
    "    \n",
    "    # Evaluate clustering\n",
    "    print(f\"\\nKMeans Clustering Results (k={n_clusters}):\")\n",
    "    print(\"-\" * 40)\n",
    "    eval_results = evaluate_clustering(true_labels, cluster_labels, algorithm_name)\n",
    "    \n",
    "    # Visualize clusters if requested\n",
    "    if visualise:\n",
    "        unique_clusters = np.unique(cluster_labels)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Create scatter plot with different colors for each cluster\n",
    "        scatter = plt.scatter(input_dataframe['Latitude'], input_dataframe['Longitude'], \n",
    "                             c=cluster_labels, cmap='viridis', s=50, alpha=0.8)\n",
    "        \n",
    "        # Add legend instead of colorbar\n",
    "        legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                          markerfacecolor=scatter.cmap(scatter.norm(i)), \n",
    "                          markersize=10, label=f'Cluster {i}') \n",
    "                          for i in unique_clusters]\n",
    "        \n",
    "        plt.legend(handles=legend_elements, title='Clusters', loc='best')\n",
    "        plt.title(plot_title)\n",
    "        plt.xlabel('Latitude')\n",
    "        plt.ylabel('Longitude')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return eval_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 1 Optimal Number of Groups\n",
    "\n",
    "Here we aim to answer what is the optimal number of groups and what effect dimensionality reduction has on clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 1a - Unique Number of Target Classes\n",
    "\n",
    "Since we have the ground truth values in a categorical value already, the ideal number of groups would be the unique number of 'sensor location' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sensor location counts\n",
    "unique_locations = df_clean['SensorLocation'].nunique()\n",
    "print(f\"Number of unique sensor locations: {unique_locations}\")\n",
    "\n",
    "location_counts = df_clean['SensorLocation'].value_counts()\n",
    "print(\"\\nUnique sensor locations and their counts:\")\n",
    "print(location_counts)\n",
    "\n",
    "# plot a bar chart of the sensor location counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "location_counts.plot(kind='bar')\n",
    "plt.title('Distribution of Sensor Locations')\n",
    "plt.xlabel('Sensor Location')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from the target feature value count that the target is imbalanced. Not a problem in clustering but for classification this would have to be handled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Scaling**\n",
    "As cluster algorithm utilise distance metrics, we need to ensure that all numeric variables are standardised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scale the numeric columns\n",
    "scaler = StandardScaler()\n",
    "df_clean[numeric_cols] = scaler.fit_transform(df_clean[numeric_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold resuls for comparison\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 1a - Optimal Cluster Count via Elbow Method\n",
    "\n",
    "Here we use the elbow method to find the optimal number of clusters. The elbow method uses the within cluster sum of squares to find the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_clusters = 15  \n",
    "\n",
    "for i in range(1, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=i, init='random', max_iter=300, n_init=10, random_state=42)\n",
    "    kmeans.fit(df_clean[numeric_cols])\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "try:\n",
    "\n",
    "    kl = KneeLocator(range(1, max_clusters + 1), wcss, curve='convex', direction='decreasing')\n",
    "    optimal_k = kl.elbow\n",
    "except ImportError:\n",
    "    diffs = [wcss[i-1] - wcss[i] for i in range(1, len(wcss))]\n",
    "    optimal_k = diffs.index(min([d for d in diffs if d > sum(diffs)/len(diffs)/2])) + 2\n",
    "\n",
    "#Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, max_clusters + 1), wcss, marker='o', linestyle='-')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, max_clusters + 1))\n",
    "\n",
    "plt.axvline(x=unique_locations, color='r', linestyle='--', \n",
    "            label=f'Ground Truth: {unique_locations} (SensorLocation)')\n",
    "plt.axvline(x=optimal_k, color='g', linestyle='--', \n",
    "            label=f'Optimal Elbow: {optimal_k}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal number of clusters determined by elbow method: {optimal_k}\")\n",
    "print(f\"Ground truth number of clusters (unique SensorLocation values): {unique_locations}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal kmeans cluster count obtained by the elbow method was 6. Compared to the ground truth count of 11. (the number of unique classes in sensor location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next perform kmeans clustering for all features using bot the elbow method and ground truth count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"K-means with {optimal_k} clusters (optimal from elbow method):\")\n",
    "perform_kmeans_clustering(df_clean, optimal_k, 'K-means')\n",
    "\n",
    "print(\"\\nK-means with 11 clusters (ground truth):\")\n",
    "perform_kmeans_clustering(df_clean, 11, 'K-means 11 clusters')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can been from this that the ground truth count of 11 clusters performed better than the elbow method of 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 1b\n",
    "\n",
    "Feature Reduction's Effect on CLustering Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic feature selection\n",
    "\n",
    "Perform automatic feature selection to find the best features for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Feature Selection for K-means Clustering\n",
    "\n",
    "# We'll use SelectKBest with mutual_info_classif to select the most informative features\n",
    "\n",
    "\n",
    "def evaluate_feature_subset(X, n_features, n_clusters=11):\n",
    "    # Select top n features\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=n_features)\n",
    "    X_selected = selector.fit_transform(X, kmeans.labels_)\n",
    "    \n",
    "    # Perform k-means clustering\n",
    "    kmeans_subset = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans_subset.fit_predict(X_selected)\n",
    "    \n",
    "    # Calculate ARI score\n",
    "    score = adjusted_rand_score(df_clean['SensorLocation'], labels)\n",
    "    \n",
    "    # Get selected feature names\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    \n",
    "    return score, selected_features\n",
    "\n",
    "# Try different numbers of features\n",
    "feature_range = range(2, len(numeric_cols) + 1)\n",
    "scores = []\n",
    "best_score = -1\n",
    "best_features = None\n",
    "\n",
    "for n_features in feature_range:\n",
    "    score, features = evaluate_feature_subset(df_clean[numeric_cols], n_features)\n",
    "    scores.append(score)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_features = features\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(feature_range, scores, marker='o')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Adjusted Rand Index')\n",
    "plt.title('Feature Selection Impact on Clustering Performance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best ARI score: {best_score:.3f}\")\n",
    "print(f\"Best performing features ({len(best_features)}):\")\n",
    "for feature in best_features:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "# Perform final clustering with best features\n",
    "X_best = df_clean[best_features]\n",
    "final_kmeans = KMeans(n_clusters=11, random_state=42)\n",
    "final_labels = final_kmeans.fit_predict(X_best)\n",
    "print(\"\\nFinal clustering performance metrics:\")\n",
    "print(f\"ARI Score: {adjusted_rand_score(df_clean['SensorLocation'], final_labels):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic feature selection showed that Longitude and Latitude were the most important features for clustering. It achieved an ARI score of 1, the highest possible score using Kmeans clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on, we will use only the latitude and longitude features for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best = df_clean[best_features + ['SensorLocation']]\n",
    "numeric_cols_best = ['Latitude', 'Longitude']\n",
    "df_best.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform kmeans on the dataframe with only latitude and longitude as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(perform_kmeans_clustering(df_best, 11, 'K-means 11 clusters', True, 'K-means 11 clusters, latitude and longitude to SensorLocation Cluster '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect score of 1 was achieved across all metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "Perform PCA to reduce the dimensionality of the data. We want to see how it compares to automatic feature selection (which produced perfect clustering results). 4 Components will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca_result = pca.fit_transform(df_clean[numeric_cols])\n",
    "pca_df = pd.DataFrame(\n",
    "    pca_result,\n",
    "    columns=['PC1', 'PC2', 'PC3', 'PC4']\n",
    ")\n",
    "\n",
    "pca_df['SensorLocation'] = df_clean['SensorLocation'].values\n",
    "\n",
    "print(\"Explained variance ratio:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"\\nCumulative explained variance ratio:\")\n",
    "print(np.cumsum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cumulative explained variance is 61.8% which is on the lower end of the spectrum. Use the principal components to perform kmeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_kmeans_clustering(pca_df, 11, 'kmeans pca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ARI  score of 0.07 was quite poor. PCA did not improve the clustering results and was worse than using the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 2\n",
    "\n",
    "In this section we will try alternate clustering algorithms other than Kmeans and shaped based algorithms to see how they perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model\n",
    "\n",
    "This is a probabilistic model that assumes all the data points are generated from a mixture of several Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=11, random_state=42)\n",
    "gmm_labels = gmm.fit_predict(df_best[numeric_cols_best])\n",
    "\n",
    "print(\"Gaussian Mixture Model Clustering Results:\")\n",
    "results.append(evaluate_clustering(df_best['SensorLocation'], gmm_labels, 'Gaussian Mixture Model'))\n",
    "\n",
    "unique_clusters = np.unique(gmm_labels)\n",
    "num_clusters = len(unique_clusters)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "scatter = plt.scatter(df_best['Latitude'], df_best['Longitude'], \n",
    "                     c=gmm_labels, cmap='viridis', s=50, alpha=0.8)\n",
    "\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                  markerfacecolor=scatter.cmap(scatter.norm(i)), \n",
    "                  markersize=10, label=f'Cluster {i}') \n",
    "                  for i in unique_clusters]\n",
    "\n",
    "plt.legend(handles=legend_elements, title='Clusters', loc='best')\n",
    "plt.title('Gaussian Mixture Model 11 Clusters, latitude and longitude to SensorLocation Cluster ')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GMM clustering algorithm performed perfected as all metrics scored the highest possible score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIRCH\n",
    "\n",
    "BIRCH is a clustering algorithm that uses a tree structure to cluster data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birch = Birch(n_clusters=11, \n",
    "                threshold=0.5, \n",
    "                branching_factor=50)  \n",
    "cluster_labels = birch.fit_predict(df_best[numeric_cols_best])\n",
    "\n",
    "results.append(evaluate_clustering(df_best['SensorLocation'], cluster_labels, 'Birch'))\n",
    "\n",
    "unique_clusters = np.unique(cluster_labels)\n",
    "num_clusters = len(unique_clusters)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "scatter = plt.scatter(df_best['Latitude'], df_best['Longitude'], \n",
    "                     c=cluster_labels, cmap='viridis', s=50, alpha=0.8)\n",
    "\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                  markerfacecolor=scatter.cmap(scatter.norm(i)), \n",
    "                  markersize=10, label=f'Cluster {i}') \n",
    "                  for i in unique_clusters]\n",
    "\n",
    "plt.legend(handles=legend_elements, title='Clusters', loc='best')\n",
    "plt.title(f'BIRCH {num_clusters} Clusters, latitude and longitude to SensorLocation Cluster ')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIRCH performed poorly with and low ARI score of 0.3115 and a low cluster count of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 3 Shape Based Clustering\n",
    "\n",
    "In this section we will try shape based clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Shape\n",
    "\n",
    "K-Shape is a clustering algorithm that finds the best match for each cluster by using the shape of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_best[numeric_cols_best].values\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "ks = KShape(n_clusters=11, random_state=42)\n",
    "cluster_labels = ks.fit_predict(X)\n",
    "\n",
    "results.append(evaluate_clustering(df_best['SensorLocation'], cluster_labels, 'K-Shape'))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "unique_clusters = np.unique(cluster_labels)\n",
    "num_clusters = len(unique_clusters)\n",
    "\n",
    "scatter = plt.scatter(df_best['Latitude'], df_best['Longitude'], \n",
    "                     c=cluster_labels, cmap='viridis', s=50, alpha=0.8)\n",
    "\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                  markerfacecolor=scatter.cmap(scatter.norm(i)), \n",
    "                  markersize=10, label=f'Cluster {i}') \n",
    "                  for i in unique_clusters]\n",
    "\n",
    "plt.legend(handles=legend_elements, title='Clusters', loc='best')\n",
    "plt.title(f'K-Shape {num_clusters} Clusters, latitude and longitude to SensorLocation Cluster')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Shape performed quite well with an ARI score of 0.7619, considering that it only found 8 clusters. Possibly this score would have been higher if more clusters were used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTICS\n",
    "\n",
    "OPTICS is desnsity based algorithm, an improvement on DBSCAN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optics = OPTICS(min_samples=5, xi=0.05, min_cluster_size=10)\n",
    "cluster_labels = optics.fit_predict(df_best[numeric_cols_best])\n",
    "\n",
    "results.append(evaluate_clustering(df_best['SensorLocation'], cluster_labels, 'OPTICS'))\n",
    "\n",
    "\n",
    "unique_clusters = np.unique(cluster_labels)\n",
    "num_clusters = len(unique_clusters[unique_clusters >= 0])  # Exclude noise points (-1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "scatter = plt.scatter(df_best['Latitude'], df_best['Longitude'], \n",
    "                     c=cluster_labels, cmap='viridis', s=50, alpha=0.8)\n",
    "\n",
    "# Add legend for each cluster (excluding noise if present)\n",
    "legend_elements = []\n",
    "for i in unique_clusters:\n",
    "    if i >= 0:  # Skip noise points for the legend\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                              markerfacecolor=scatter.cmap(scatter.norm(i)), \n",
    "                              markersize=10, label=f'Cluster {i}'))\n",
    "    elif i == -1:  # Add a special entry for noise points\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                              markerfacecolor='lightgrey', \n",
    "                              markersize=10, label='Noise'))\n",
    "\n",
    "plt.legend(handles=legend_elements, title='Clusters', loc='best')\n",
    "plt.title(f'OPTICS Clustering - Geographical Distribution')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTICS performed extremely well considering it found 12 clusters. Its ARI score was just shy of perfect coming in at 0.9717"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 4 Best Solution\n",
    "\n",
    "Here we will compare the performance of all the clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results table\n",
    "print(\"Clustering Algorithm Performance Comparison:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Optional: Sort by a specific metric (e.g., ARI) to see best performing algorithms\n",
    "print(\"\\nAlgorithms sorted by ARI (best to worst):\")\n",
    "print(results_df.sort_values('ari', ascending=False).to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans and GMM performed the best with the highest ARI scores. OPTICS was a close third though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 5 Relationship Amongst Independent Variables\n",
    "\n",
    "Produce a correlation matrix to see the relationship between the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "correlation_matrix = df_clean.drop(columns=['SensorLocation', 'Device_id', 'Time']).corr()\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Heatmap of the correlation matrix\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f')\n",
    "plt.title('Correlation Matrix of Environmental Variables', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# find the strongest positive and negative correlations\n",
    "corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], \n",
    "                          correlation_matrix.iloc[i, j]))\n",
    "\n",
    "corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "print(\"\\nTop 10 Strongest Correlations:\")\n",
    "for var1, var2, corr in corr_pairs[:10]:\n",
    "    print(f\"{var1} and {var2}: {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the independent variable had strong positive and negative correlations. This is discussed in the report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
